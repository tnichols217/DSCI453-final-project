"""
This type stub file was generated by pyright.
"""

from tensorflow.security.fuzzing.py import annotation_types as _atypes
from tensorflow.python.util import dispatch as _dispatch
from tensorflow.python.util.tf_export import tf_export
from typing import Any, TypeVar
from typing_extensions import Annotated

"""Python wrappers around TensorFlow ops.

This file is MACHINE GENERATED! Do not edit.
"""
@_dispatch.add_fallback_dispatch_list
@_dispatch.add_type_based_api_dispatcher
@tf_export('configure_and_initialize_global_tpu')
def configure_and_initialize_global_tpu(use_tfrt_host_runtime: bool = ..., name=...) -> Annotated[Any, _atypes.Int32]:
  r"""TODO: add doc.

  Args:
    use_tfrt_host_runtime: An optional `bool`. Defaults to `True`.
    name: A name for the operation (optional).

  Returns:
    A `Tensor` of type `int32`.
  """
  ...

ConfigureAndInitializeGlobalTPU = ...
_dispatcher_for_configure_and_initialize_global_tpu = configure_and_initialize_global_tpu._tf_type_based_dispatcher.Dispatch
def configure_and_initialize_global_tpu_eager_fallback(use_tfrt_host_runtime: bool, name, ctx) -> Annotated[Any, _atypes.Int32]:
  ...

TV_CopyToMesh_T = TypeVar("TV_CopyToMesh_T", _atypes.BFloat16, _atypes.Bool, _atypes.Complex128, _atypes.Complex64, _atypes.Float16, _atypes.Float32, _atypes.Float64, _atypes.Float8e4m3fn, _atypes.Float8e5m2, _atypes.Half, _atypes.Int16, _atypes.Int32, _atypes.Int4, _atypes.Int64, _atypes.Int8, _atypes.QInt16, _atypes.QInt32, _atypes.QInt8, _atypes.QUInt16, _atypes.QUInt8, _atypes.Resource, _atypes.String, _atypes.UInt16, _atypes.UInt32, _atypes.UInt4, _atypes.UInt64, _atypes.UInt8, _atypes.Variant)
@_dispatch.add_fallback_dispatch_list
@_dispatch.add_type_based_api_dispatcher
@tf_export('copy_to_mesh')
def copy_to_mesh(input: Annotated[Any, TV_CopyToMesh_T], mesh: str, name=...) -> Annotated[Any, TV_CopyToMesh_T]:
  r"""TODO: add doc.

  Args:
    input: A `Tensor`.
    mesh: A `string`.
    name: A name for the operation (optional).

  Returns:
    A `Tensor`. Has the same type as `input`.
  """
  ...

CopyToMesh = ...
_dispatcher_for_copy_to_mesh = copy_to_mesh._tf_type_based_dispatcher.Dispatch
def copy_to_mesh_eager_fallback(input: Annotated[Any, TV_CopyToMesh_T], mesh: str, name, ctx) -> Annotated[Any, TV_CopyToMesh_T]:
  ...

TV_CopyToMeshGrad_T = TypeVar("TV_CopyToMeshGrad_T", _atypes.BFloat16, _atypes.Bool, _atypes.Complex128, _atypes.Complex64, _atypes.Float16, _atypes.Float32, _atypes.Float64, _atypes.Float8e4m3fn, _atypes.Float8e5m2, _atypes.Half, _atypes.Int16, _atypes.Int32, _atypes.Int4, _atypes.Int64, _atypes.Int8, _atypes.QInt16, _atypes.QInt32, _atypes.QInt8, _atypes.QUInt16, _atypes.QUInt8, _atypes.Resource, _atypes.String, _atypes.UInt16, _atypes.UInt32, _atypes.UInt4, _atypes.UInt64, _atypes.UInt8, _atypes.Variant)
@_dispatch.add_fallback_dispatch_list
@_dispatch.add_type_based_api_dispatcher
@tf_export('copy_to_mesh_grad')
def copy_to_mesh_grad(input: Annotated[Any, TV_CopyToMeshGrad_T], forward_input: Annotated[Any, TV_CopyToMeshGrad_T], name=...) -> Annotated[Any, TV_CopyToMeshGrad_T]:
  r"""TODO: add doc.

  Args:
    input: A `Tensor`.
    forward_input: A `Tensor`. Must have the same type as `input`.
    name: A name for the operation (optional).

  Returns:
    A `Tensor`. Has the same type as `input`.
  """
  ...

CopyToMeshGrad = ...
_dispatcher_for_copy_to_mesh_grad = copy_to_mesh_grad._tf_type_based_dispatcher.Dispatch
def copy_to_mesh_grad_eager_fallback(input: Annotated[Any, TV_CopyToMeshGrad_T], forward_input: Annotated[Any, TV_CopyToMeshGrad_T], name, ctx) -> Annotated[Any, TV_CopyToMeshGrad_T]:
  ...

@_dispatch.add_fallback_dispatch_list
@_dispatch.add_type_based_api_dispatcher
@tf_export('d_tensor_restore_v2')
def d_tensor_restore_v2(prefix: Annotated[Any, _atypes.String], tensor_names: Annotated[Any, _atypes.String], shape_and_slices: Annotated[Any, _atypes.String], input_shapes, input_layouts, dtypes, name=...): # -> object | _dispatcher_for_d_tensor_restore_v2 | Operation | tuple[Any, ...] | list[Any]:
  r"""TODO: add doc.

  Args:
    prefix: A `Tensor` of type `string`.
    tensor_names: A `Tensor` of type `string`.
    shape_and_slices: A `Tensor` of type `string`.
    input_shapes: A list of shapes (each a `tf.TensorShape` or list of `ints`).
    input_layouts: A list of `strings`.
    dtypes: A list of `tf.DTypes` that has length `>= 1`.
    name: A name for the operation (optional).

  Returns:
    A list of `Tensor` objects of type `dtypes`.
  """
  ...

DTensorRestoreV2 = ...
_dispatcher_for_d_tensor_restore_v2 = d_tensor_restore_v2._tf_type_based_dispatcher.Dispatch
def d_tensor_restore_v2_eager_fallback(prefix: Annotated[Any, _atypes.String], tensor_names: Annotated[Any, _atypes.String], shape_and_slices: Annotated[Any, _atypes.String], input_shapes, input_layouts, dtypes, name, ctx): # -> object:
  ...

@_dispatch.add_fallback_dispatch_list
@_dispatch.add_type_based_api_dispatcher
@tf_export('d_tensor_set_global_tpu_array')
def d_tensor_set_global_tpu_array(topology: Annotated[Any, _atypes.String], name=...): # -> object | _dispatcher_for_d_tensor_set_global_tpu_array | Operation | None:
  r"""TODO: add doc.

  Args:
    topology: A `Tensor` of type `string`.
    name: A name for the operation (optional).

  Returns:
    The created Operation.
  """
  ...

DTensorSetGlobalTPUArray = ...
_dispatcher_for_d_tensor_set_global_tpu_array = d_tensor_set_global_tpu_array._tf_type_based_dispatcher.Dispatch
def d_tensor_set_global_tpu_array_eager_fallback(topology: Annotated[Any, _atypes.String], name, ctx): # -> None:
  ...

TV_Relayout_T = TypeVar("TV_Relayout_T", _atypes.BFloat16, _atypes.Bool, _atypes.Complex128, _atypes.Complex64, _atypes.Float16, _atypes.Float32, _atypes.Float64, _atypes.Float8e4m3fn, _atypes.Float8e5m2, _atypes.Half, _atypes.Int16, _atypes.Int32, _atypes.Int4, _atypes.Int64, _atypes.Int8, _atypes.QInt16, _atypes.QInt32, _atypes.QInt8, _atypes.QUInt16, _atypes.QUInt8, _atypes.Resource, _atypes.String, _atypes.UInt16, _atypes.UInt32, _atypes.UInt4, _atypes.UInt64, _atypes.UInt8, _atypes.Variant)
@_dispatch.add_fallback_dispatch_list
@_dispatch.add_type_based_api_dispatcher
@tf_export('relayout')
def relayout(input: Annotated[Any, TV_Relayout_T], layout: str, name=...) -> Annotated[Any, TV_Relayout_T]:
  r"""TODO: add doc.

  Args:
    input: A `Tensor`.
    layout: A `string`.
    name: A name for the operation (optional).

  Returns:
    A `Tensor`. Has the same type as `input`.
  """
  ...

Relayout = ...
_dispatcher_for_relayout = relayout._tf_type_based_dispatcher.Dispatch
def relayout_eager_fallback(input: Annotated[Any, TV_Relayout_T], layout: str, name, ctx) -> Annotated[Any, TV_Relayout_T]:
  ...

TV_RelayoutLike_T = TypeVar("TV_RelayoutLike_T", _atypes.BFloat16, _atypes.Bool, _atypes.Complex128, _atypes.Complex64, _atypes.Float16, _atypes.Float32, _atypes.Float64, _atypes.Float8e4m3fn, _atypes.Float8e5m2, _atypes.Half, _atypes.Int16, _atypes.Int32, _atypes.Int4, _atypes.Int64, _atypes.Int8, _atypes.QInt16, _atypes.QInt32, _atypes.QInt8, _atypes.QUInt16, _atypes.QUInt8, _atypes.Resource, _atypes.String, _atypes.UInt16, _atypes.UInt32, _atypes.UInt4, _atypes.UInt64, _atypes.UInt8, _atypes.Variant)
TV_RelayoutLike_U = TypeVar("TV_RelayoutLike_U", _atypes.BFloat16, _atypes.Bool, _atypes.Complex128, _atypes.Complex64, _atypes.Float16, _atypes.Float32, _atypes.Float64, _atypes.Float8e4m3fn, _atypes.Float8e5m2, _atypes.Half, _atypes.Int16, _atypes.Int32, _atypes.Int4, _atypes.Int64, _atypes.Int8, _atypes.QInt16, _atypes.QInt32, _atypes.QInt8, _atypes.QUInt16, _atypes.QUInt8, _atypes.Resource, _atypes.String, _atypes.UInt16, _atypes.UInt32, _atypes.UInt4, _atypes.UInt64, _atypes.UInt8, _atypes.Variant)
@_dispatch.add_fallback_dispatch_list
@_dispatch.add_type_based_api_dispatcher
@tf_export('relayout_like')
def relayout_like(input: Annotated[Any, TV_RelayoutLike_T], layout_input: Annotated[Any, TV_RelayoutLike_U], name=...) -> Annotated[Any, TV_RelayoutLike_T]:
  r"""TODO: add doc.

  Args:
    input: A `Tensor`.
    layout_input: A `Tensor`.
    name: A name for the operation (optional).

  Returns:
    A `Tensor`. Has the same type as `input`.
  """
  ...

RelayoutLike = ...
_dispatcher_for_relayout_like = relayout_like._tf_type_based_dispatcher.Dispatch
def relayout_like_eager_fallback(input: Annotated[Any, TV_RelayoutLike_T], layout_input: Annotated[Any, TV_RelayoutLike_U], name, ctx) -> Annotated[Any, TV_RelayoutLike_T]:
  ...

@_dispatch.add_fallback_dispatch_list
@_dispatch.add_type_based_api_dispatcher
@tf_export('shutdown_tpu_system')
def shutdown_tpu_system(name=...) -> Annotated[Any, _atypes.Bool]:
  r"""TODO: add doc.

  Args:
    name: A name for the operation (optional).

  Returns:
    A `Tensor` of type `bool`.
  """
  ...

ShutdownTPUSystem = ...
_dispatcher_for_shutdown_tpu_system = shutdown_tpu_system._tf_type_based_dispatcher.Dispatch
def shutdown_tpu_system_eager_fallback(name, ctx) -> Annotated[Any, _atypes.Bool]:
  ...

