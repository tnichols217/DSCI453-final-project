"""
This type stub file was generated by pyright.
"""

from typing import Dict, List, Optional
from tensorflow.dtensor.python import layout as layout_lib
from tensorflow.python.util.tf_export import tf_export

"""TPU-specific utilities for DTensor."""
_MESH_DIM_X = ...
_TPU_DEVICE_TYPE = ...
_dtensor_device = ...
_tpu_topology = ...
_all_core_ids = ...
_all_core_locations = ...
class _CoreLocation:
  """Represents a TPU core's location in the mesh."""
  def __init__(self, x: int = ..., y: int = ..., z: int = ..., core: int = ...) -> None:
    ...
  
  def __eq__(self, other) -> bool:
    ...
  
  def __ne__(self, other) -> bool:
    ...
  
  def __hash__(self) -> int:
    ...
  
  def __repr__(self): # -> str:
    ...
  
  def to_list(self): # -> list[int]:
    ...
  


def shutdown_tpu_system(): # -> None:
  """Shuts down the TPU system."""
  ...

def tpu_system_init_helper(task_id, num_tasks, num_devices, use_tfrt_host_runtime=..., use_megacore=...): # -> tuple[Topology, DTensorDevice]:
  """A helper function to initialize multi-client tpu system."""
  ...

def initialize_tpu_system(use_megacore=...): # -> None:
  """Initializes the TPU system."""
  ...

@tf_export("experimental.dtensor.create_tpu_mesh", v1=[])
def create_tpu_mesh(mesh_dim_names: List[str], mesh_shape: List[int], mesh_name: str, ring_dims: Optional[int] = ..., ring_axes: Optional[List[str]] = ..., ring_bounds: Optional[List[int]] = ..., can_split_host_across_rings: bool = ..., build_ring_across_rings: bool = ..., rotate_ring_across_rings: bool = ..., use_xla_spmd: bool = ...) -> layout_lib.Mesh:
  """Returns a distributed TPU mesh optimized for AllReduce ring reductions.

  Only as many as leading axes specified by `ring_axes` as necessary will be
  used to build rings, as long as the subslice formed by these axes have enough
  cores to contain a ring of the required size. The leftover axes in `ring_axes`
  won't affect results.

  This function always uses all TPU devices, and offers more customization than
  `tf.experimental.dtensor.create_distributed_mesh`.

  Args:
    mesh_dim_names: List of mesh dimension names.
    mesh_shape: Shape of the mesh.
    mesh_name: A unique name for the mesh. If empty, internally generate one.
    ring_dims: Optional; The number of leading (ring_dims > 0) or trailing
      (ring_dims < 0) mesh dimensions to build rings for. If unspecified, build
      rings for all but the first dimension.
    ring_axes: Optional; A permutation of ["x", "y", "z", "core"], specifying
      the order of TPU topology axes to build rings in. If unspecified, default
      to ["core", "x", "y", "z"].
    ring_bounds: Optional; The maximum number of devices on each axis, in the x,
      y, z, core order. If unspecified, default to physical topology limits.
    can_split_host_across_rings: Optional; If true, devices attached to the same
      host (i.e., DTensor client) may get assigned to different rings. Setting
      it to false may cause some combinations of arguments to be infeasible; see
      DeviceAssignmentTest.testCreateMesh[No]SplittingHosts* for examples.
    build_ring_across_rings: Optional; If true, also build a data-parallel ring
      across model-parallel rings. This ring could be strided.
    rotate_ring_across_rings: Optional; If true, build the data-parallel ring in
      column-major instead of row-major order.
    use_xla_spmd: Boolean when True, will use XLA SPMD instead of
      DTensor SPMD.
  """
  ...

def get_device_ids(mesh: layout_lib.Mesh, client_id: Optional[int] = ...) -> List[int]:
  """Returns the device IDs of all TPU cores local to the given client.

  A device ID is a non-negative integer that uniquely identifies a device in the
  mesh. For example, for a 2x2 mesh ('x', 'y'), this function returns a
  permutation of [0, 1, 2, 3].

  Note that device IDs and device locations are equivalent. The former is a
  linearization of the latter along mesh dimensions.

  Args:
    mesh: A TPU mesh.
    client_id: Optional; A DTensor client ID. If empty, query this client.
  """
  ...

def get_device_locations(mesh: layout_lib.Mesh, client_id: Optional[int] = ...) -> List[Dict[str, int]]:
  """Returns the device locations of all TPU cores local to the given client.

  A device location is a dictionary from dimension names to indices on those
  dimensions. For example, for a 2x2 mesh ('x', 'y'), this function returns a
  permutation of this list:

    [{'x': 0, 'y': 0},
     {'x': 0, 'y': 1},
     {'x': 1, 'y': 0},
     {'x': 1, 'y': 1}].

  Note that device IDs and device locations are equivalent. The former is a
  linearization of the latter along mesh dimensions.

  Args:
    mesh: A TPU mesh.
    client_id: Optional; A DTensor client ID. If empty, query this client.
  """
  ...

def dtensor_initialize_tpu_system(enable_coordination_service=...): # -> None:
  """Deprecated way to initialize the TPU system."""
  ...

def dtensor_shutdown_tpu_system(): # -> None:
  """Deprecated way to shutodwn the TPU system."""
  ...

