"""
This type stub file was generated by pyright.
"""

from typing import MutableSequence, Sequence
from tensorflow.python.checkpoint.sharding import sharding_util
from tensorflow.python.framework import device as device_lib, dtypes, tensor as tensor_lib, tensor_shape
from tensorflow.python.ops import variables
from tensorflow.python.util import tf_export

"""Checkpoint policies that determine how tensors are split into shards."""
@tf_export.tf_export("train.experimental.ShardByTaskPolicy")
class ShardByTaskPolicy(sharding_util.ShardingCallback):
  """Policy that splits tensors into shards based on their device spec task."""
  @property
  def description(self) -> str:
    ...
  
  def __call__(self, shardable_tensors: Sequence[sharding_util.ShardableTensor]) -> Sequence[sharding_util.Shard]:
    """Callback to split tensors into shards based on their device spec task.

    Args:
      shardable_tensors: A list of ShardableTensors.

    Returns:
      List of shard dicts containing tensors.
          [ {checkpoint key: {slice_spec: tensor} } ]
    """
    ...
  


_OffsetAndShape = tuple[Sequence[int], Sequence[int]]
@tf_export.tf_export("train.experimental.MaxShardSizePolicy")
class MaxShardSizePolicy(sharding_util.ShardingCallback):
  """Policy that splits tensors into shards with a max shard size.

  Shards may exceed the max shard size if they contain 1. a single scalar/string
  tensor that could not be sliced and exceeds the max shard size or 2. the
  checkpoint object graph, whose size cannot be calculated when saving.
  """
  class MaxShardSizePartitioner:
    """Partition tensors into shards with a max shard size."""
    max_shard_size: int
    _large_scalars: MutableSequence[sharding_util.Shard]
    _tensors_by_shard: MutableSequence[sharding_util.Shard]
    _shard_size_remaining: int
    _checkpoint_key: str
    _dtype: dtypes.DType
    _device: device_lib.DeviceSpec
    _root_tensor: tensor_lib.Tensor
    _slice_spec: variables.Variable.SaveSliceInfo
    _full_shape: tensor_shape.TensorShape
    _root_shape: tensor_shape.TensorShape
    _root_offset: Sequence[int]
    _dtype_size: int
    _working_tensor_offset: MutableSequence[float]
    _working_tensor_shape: tensor_shape.TensorShape
    def get_shards(self, max_shard_size: int, shardable_tensors: Sequence[sharding_util.ShardableTensor]) -> Sequence[sharding_util.Shard]:
      """Callback to split tensors into shards with a max shard size.

      Args:
        max_shard_size: The maximum size of a shard file in bytes.
        shardable_tensors: A list of ShardableTensors.

      Returns:
        List of shard dicts containing tensors.
            [ {checkpoint key: {slice_spec: tensor} } ]
      """
      ...
    
  
  
  def __init__(self, max_shard_size: int) -> None:
    ...
  
  @property
  def description(self) -> str:
    ...
  
  def __call__(self, shardable_tensors: Sequence[sharding_util.ShardableTensor]) -> Sequence[sharding_util.Shard]:
    ...
  


