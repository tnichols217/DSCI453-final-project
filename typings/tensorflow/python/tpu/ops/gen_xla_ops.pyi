"""
This type stub file was generated by pyright.
"""

from tensorflow.security.fuzzing.py import annotation_types as _atypes
from typing import Any, List
from typing_extensions import Annotated

"""Python wrappers around TensorFlow ops.

This file is MACHINE GENERATED! Do not edit.
"""
_ConvertToCooTensorOutput = ...
def convert_to_coo_tensor(indices_or_row_splits: Annotated[Any, _atypes.Int32], values: Annotated[Any, _atypes.Int32], weights: Annotated[Any, _atypes.Float32], sample_count: int, combiner: str, name=...): # -> ConvertToCooTensor:
  r"""TODO: add doc.

  Args:
    indices_or_row_splits: A `Tensor` of type `int32`.
    values: A `Tensor` of type `int32`.
    weights: A `Tensor` of type `float32`.
    sample_count: An `int` that is `>= 1`.
    combiner: A `string`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (row_ids, col_ids, gains).

    row_ids: A `Tensor` of type `int32`.
    col_ids: A `Tensor` of type `int32`.
    gains: A `Tensor` of type `float32`.
  """
  ...

ConvertToCooTensor = ...
def convert_to_coo_tensor_eager_fallback(indices_or_row_splits: Annotated[Any, _atypes.Int32], values: Annotated[Any, _atypes.Int32], weights: Annotated[Any, _atypes.Float32], sample_count: int, combiner: str, name, ctx): # -> ConvertToCooTensor:
  ...

_ConvertToListOfSparseCoreCooTensorsOutput = ...
def convert_to_list_of_sparse_core_coo_tensors(indices_or_row_splits: Annotated[Any, _atypes.Int32], values: Annotated[Any, _atypes.Int32], weights: Annotated[Any, _atypes.Float32], sample_count: int, num_sc_per_chip: int, row_offset: int, col_offset: int, col_shift: int, num_sc_shards: int, stacked_table_sample_count: int, combiner: str, name=...): # -> ConvertToListOfSparseCoreCooTensors:
  r"""TODO: add doc.

  Args:
    indices_or_row_splits: A `Tensor` of type `int32`.
    values: A `Tensor` of type `int32`.
    weights: A `Tensor` of type `float32`.
    sample_count: An `int` that is `>= 1`.
    num_sc_per_chip: An `int` that is `>= 1`.
    row_offset: An `int` that is `>= 0`.
    col_offset: An `int` that is `>= 0`.
    col_shift: An `int` that is `>= 0`.
    num_sc_shards: An `int` that is `>= 1`.
    stacked_table_sample_count: An `int` that is `>= 1`.
    combiner: A `string`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (row_ids_list, col_ids_list, gains_list).

    row_ids_list: A list of `num_sc_per_chip` `Tensor` objects with type `int32`.
    col_ids_list: A list of `num_sc_per_chip` `Tensor` objects with type `int32`.
    gains_list: A list of `num_sc_per_chip` `Tensor` objects with type `float32`.
  """
  ...

ConvertToListOfSparseCoreCooTensors = ...
def convert_to_list_of_sparse_core_coo_tensors_eager_fallback(indices_or_row_splits: Annotated[Any, _atypes.Int32], values: Annotated[Any, _atypes.Int32], weights: Annotated[Any, _atypes.Float32], sample_count: int, num_sc_per_chip: int, row_offset: int, col_offset: int, col_shift: int, num_sc_shards: int, stacked_table_sample_count: int, combiner: str, name, ctx): # -> ConvertToListOfSparseCoreCooTensors:
  ...

_ConvertToSparseCoreCsrWrappedCooTensorOutput = ...
def convert_to_sparse_core_csr_wrapped_coo_tensor(sorted_row_ids_list: Annotated[List[Any], _atypes.Int32], sorted_col_ids_list: Annotated[List[Any], _atypes.Int32], sorted_gains_list: Annotated[List[Any], _atypes.Float32], id_counts_list: Annotated[List[Any], _atypes.Int32], splits: Annotated[Any, _atypes.Int64], sample_count_per_sc: int, num_replica: int, max_minibatches_per_sc: int, max_ids_per_chip_per_sample: int, table_vocab_size: int, feature_width: int, table_name: str, allow_id_dropping: bool, name=...): # -> ConvertToSparseCoreCsrWrappedCooTensor:
  r"""TODO: add doc.

  Args:
    sorted_row_ids_list: A list of at least 1 `Tensor` objects with type `int32`.
    sorted_col_ids_list: A list with the same length as `sorted_row_ids_list` of `Tensor` objects with type `int32`.
    sorted_gains_list: A list with the same length as `sorted_row_ids_list` of `Tensor` objects with type `float32`.
    id_counts_list: A list with the same length as `sorted_row_ids_list` of `Tensor` objects with type `int32`.
    splits: A `Tensor` of type `int64`.
    sample_count_per_sc: An `int` that is `>= 1`.
    num_replica: An `int` that is `>= 1`.
    max_minibatches_per_sc: An `int` that is `>= 1`.
    max_ids_per_chip_per_sample: An `int` that is `>= 1`.
    table_vocab_size: An `int` that is `>= 1`.
    feature_width: An `int` that is `>= 1`.
    table_name: A `string`.
    allow_id_dropping: A `bool`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (row_pointers, sorted_sample_ids, sorted_token_ids, sorted_gains, row_pointers_unpadded_size, ids_unpadded_size, num_minibatches_per_sc).

    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    row_pointers_unpadded_size: A `Tensor` of type `int32`.
    ids_unpadded_size: A `Tensor` of type `int32`.
    num_minibatches_per_sc: A `Tensor` of type `int32`.
  """
  ...

ConvertToSparseCoreCsrWrappedCooTensor = ...
def convert_to_sparse_core_csr_wrapped_coo_tensor_eager_fallback(sorted_row_ids_list: Annotated[List[Any], _atypes.Int32], sorted_col_ids_list: Annotated[List[Any], _atypes.Int32], sorted_gains_list: Annotated[List[Any], _atypes.Float32], id_counts_list: Annotated[List[Any], _atypes.Int32], splits: Annotated[Any, _atypes.Int64], sample_count_per_sc: int, num_replica: int, max_minibatches_per_sc: int, max_ids_per_chip_per_sample: int, table_vocab_size: int, feature_width: int, table_name: str, allow_id_dropping: bool, name, ctx): # -> ConvertToSparseCoreCsrWrappedCooTensor:
  ...

_GetMinibatchSplitsWithPhysicalReplicaOutput = ...
def get_minibatch_splits_with_physical_replica(program_key: Annotated[Any, _atypes.String], row_ids: Annotated[Any, _atypes.Int32], col_ids: Annotated[Any, _atypes.Int32], gains: Annotated[Any, _atypes.Float32], sample_count: int, num_replica: int, table_vocab_size: int, feature_width: int, num_sc_per_chip: int, table_name: str, mini_batch_splits: str, name=...): # -> GetMinibatchSplitsWithPhysicalReplica:
  r"""TODO: add doc.

  Args:
    program_key: A `Tensor` of type `string`.
    row_ids: A `Tensor` of type `int32`.
    col_ids: A `Tensor` of type `int32`.
    gains: A `Tensor` of type `float32`.
    sample_count: An `int` that is `>= 1`.
    num_replica: An `int` that is `>= 1`.
    table_vocab_size: An `int` that is `>= 1`.
    feature_width: An `int` that is `>= 1`.
    num_sc_per_chip: An `int` that is `>= 1`.
    table_name: A `string`.
    mini_batch_splits: A `string`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (sorted_row_ids, sorted_col_ids, sorted_gains, splits, id_counts, max_ids, max_uniques).

    sorted_row_ids: A `Tensor` of type `int32`.
    sorted_col_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    splits: A `Tensor` of type `int64`.
    id_counts: A `Tensor` of type `int32`.
    max_ids: A `Tensor` of type `int32`.
    max_uniques: A `Tensor` of type `int32`.
  """
  ...

GetMinibatchSplitsWithPhysicalReplica = ...
def get_minibatch_splits_with_physical_replica_eager_fallback(program_key: Annotated[Any, _atypes.String], row_ids: Annotated[Any, _atypes.Int32], col_ids: Annotated[Any, _atypes.Int32], gains: Annotated[Any, _atypes.Float32], sample_count: int, num_replica: int, table_vocab_size: int, feature_width: int, num_sc_per_chip: int, table_name: str, mini_batch_splits: str, name, ctx): # -> GetMinibatchSplitsWithPhysicalReplica:
  ...

_GetMinibatchesInCsrWithPhysicalReplicaOutput = ...
def get_minibatches_in_csr_with_physical_replica(program_key: Annotated[Any, _atypes.String], row_ids: Annotated[Any, _atypes.Int32], col_ids: Annotated[Any, _atypes.Int32], gains: Annotated[Any, _atypes.Float32], splits: Annotated[Any, _atypes.Int64], id_counts: Annotated[Any, _atypes.Int32], sample_count: int, num_replica: int, max_minibatches_per_sc: int, max_ids_per_chip_per_sample: int, table_vocab_size: int, feature_width: int, num_sc_per_chip: int, table_name: str, mini_batch_in_csr: str, name=...): # -> GetMinibatchesInCsrWithPhysicalReplica:
  r"""TODO: add doc.

  Args:
    program_key: A `Tensor` of type `string`.
    row_ids: A `Tensor` of type `int32`.
    col_ids: A `Tensor` of type `int32`.
    gains: A `Tensor` of type `float32`.
    splits: A `Tensor` of type `int64`.
    id_counts: A `Tensor` of type `int32`.
    sample_count: An `int` that is `>= 1`.
    num_replica: An `int` that is `>= 1`.
    max_minibatches_per_sc: An `int` that is `>= 1`.
    max_ids_per_chip_per_sample: An `int` that is `>= 1`.
    table_vocab_size: An `int` that is `>= 1`.
    feature_width: An `int` that is `>= 1`.
    num_sc_per_chip: An `int` that is `>= 1`.
    table_name: A `string`.
    mini_batch_in_csr: A `string`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (row_pointers, sorted_sample_ids, sorted_token_ids, sorted_gains, row_pointers_unpadded_size, ids_unpadded_size, num_minibatches_per_physical_sparse_core).

    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    row_pointers_unpadded_size: A `Tensor` of type `int32`.
    ids_unpadded_size: A `Tensor` of type `int32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
  """
  ...

GetMinibatchesInCsrWithPhysicalReplica = ...
def get_minibatches_in_csr_with_physical_replica_eager_fallback(program_key: Annotated[Any, _atypes.String], row_ids: Annotated[Any, _atypes.Int32], col_ids: Annotated[Any, _atypes.Int32], gains: Annotated[Any, _atypes.Float32], splits: Annotated[Any, _atypes.Int64], id_counts: Annotated[Any, _atypes.Int32], sample_count: int, num_replica: int, max_minibatches_per_sc: int, max_ids_per_chip_per_sample: int, table_vocab_size: int, feature_width: int, num_sc_per_chip: int, table_name: str, mini_batch_in_csr: str, name, ctx): # -> GetMinibatchesInCsrWithPhysicalReplica:
  ...

_GetStatsFromListOfSparseCoreCooTensorsOutput = ...
def get_stats_from_list_of_sparse_core_coo_tensors(row_ids_list: Annotated[List[Any], _atypes.Int32], col_ids_list: Annotated[List[Any], _atypes.Int32], gains_list: Annotated[List[Any], _atypes.Float32], sample_count_list, col_offset_list, num_replica: int, table_vocab_size: int, feature_width: int, num_sc_per_chip: int, table_name: str, name=...): # -> GetStatsFromListOfSparseCoreCooTensors:
  r"""TODO: add doc.

  Args:
    row_ids_list: A list of at least 1 `Tensor` objects with type `int32`.
    col_ids_list: A list with the same length as `row_ids_list` of `Tensor` objects with type `int32`.
    gains_list: A list with the same length as `row_ids_list` of `Tensor` objects with type `float32`.
    sample_count_list: A list of `ints`.
    col_offset_list: A list of `ints`.
    num_replica: An `int` that is `>= 1`.
    table_vocab_size: An `int` that is `>= 1`.
    feature_width: An `int` that is `>= 1`.
    num_sc_per_chip: An `int` that is `>= 1`.
    table_name: A `string`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (max_ids_per_sparse_core, max_unique_ids_per_sparse_core).

    max_ids_per_sparse_core: A `Tensor` of type `int32`.
    max_unique_ids_per_sparse_core: A `Tensor` of type `int32`.
  """
  ...

GetStatsFromListOfSparseCoreCooTensors = ...
def get_stats_from_list_of_sparse_core_coo_tensors_eager_fallback(row_ids_list: Annotated[List[Any], _atypes.Int32], col_ids_list: Annotated[List[Any], _atypes.Int32], gains_list: Annotated[List[Any], _atypes.Float32], sample_count_list, col_offset_list, num_replica: int, table_vocab_size: int, feature_width: int, num_sc_per_chip: int, table_name: str, name, ctx): # -> GetStatsFromListOfSparseCoreCooTensors:
  ...

def global_iter_id(name=...) -> Annotated[Any, _atypes.Int64]:
  r"""TODO: add doc.

  Args:
    name: A name for the operation (optional).

  Returns:
    A `Tensor` of type `int64`.
  """
  ...

GlobalIterId = ...
def global_iter_id_eager_fallback(name, ctx) -> Annotated[Any, _atypes.Int64]:
  ...

_SortListOfSparseCoreCooTensorsOutput = ...
def sort_list_of_sparse_core_coo_tensors(row_ids_list: Annotated[List[Any], _atypes.Int32], col_ids_list: Annotated[List[Any], _atypes.Int32], gains_list: Annotated[List[Any], _atypes.Float32], sample_count_list, col_offset_list, num_replica: int, table_vocab_size: int, feature_width: int, num_sc_per_chip: int, max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, name=...): # -> SortListOfSparseCoreCooTensors:
  r"""TODO: add doc.

  Args:
    row_ids_list: A list of at least 1 `Tensor` objects with type `int32`.
    col_ids_list: A list with the same length as `row_ids_list` of `Tensor` objects with type `int32`.
    gains_list: A list with the same length as `row_ids_list` of `Tensor` objects with type `float32`.
    sample_count_list: A list of `ints`.
    col_offset_list: A list of `ints`.
    num_replica: An `int` that is `>= 1`.
    table_vocab_size: An `int` that is `>= 1`.
    feature_width: An `int` that is `>= 1`.
    num_sc_per_chip: An `int` that is `>= 1`.
    max_ids_per_sparse_core: An `int` that is `>= 1`.
    max_unique_ids_per_sparse_core: An `int` that is `>= 1`.
    table_name: A `string`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (sorted_row_ids, sorted_col_ids, sorted_gains, id_counts).

    sorted_row_ids: A `Tensor` of type `int32`.
    sorted_col_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    id_counts: A `Tensor` of type `int32`.
  """
  ...

SortListOfSparseCoreCooTensors = ...
def sort_list_of_sparse_core_coo_tensors_eager_fallback(row_ids_list: Annotated[List[Any], _atypes.Int32], col_ids_list: Annotated[List[Any], _atypes.Int32], gains_list: Annotated[List[Any], _atypes.Float32], sample_count_list, col_offset_list, num_replica: int, table_vocab_size: int, feature_width: int, num_sc_per_chip: int, max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, name, ctx): # -> SortListOfSparseCoreCooTensors:
  ...

def store_minibatch_statistics_in_fdo(program_key: Annotated[Any, _atypes.String], max_ids: Annotated[Any, _atypes.Int32], max_uniques: Annotated[Any, _atypes.Int32], sample_count: int, num_replica: int, feature_width: int, num_sc_per_chip: int, table_name: str, mini_batch_splits: str, name=...): # -> object | Operation | None:
  r"""TODO: add doc.

  Args:
    program_key: A `Tensor` of type `string`.
    max_ids: A `Tensor` of type `int32`.
    max_uniques: A `Tensor` of type `int32`.
    sample_count: An `int` that is `>= 1`.
    num_replica: An `int` that is `>= 1`.
    feature_width: An `int` that is `>= 1`.
    num_sc_per_chip: An `int` that is `>= 1`.
    table_name: A `string`.
    mini_batch_splits: A `string`.
    name: A name for the operation (optional).

  Returns:
    The created Operation.
  """
  ...

StoreMinibatchStatisticsInFdo = ...
def store_minibatch_statistics_in_fdo_eager_fallback(program_key: Annotated[Any, _atypes.String], max_ids: Annotated[Any, _atypes.Int32], max_uniques: Annotated[Any, _atypes.Int32], sample_count: int, num_replica: int, feature_width: int, num_sc_per_chip: int, table_name: str, mini_batch_splits: str, name, ctx): # -> None:
  ...

def tpu_annotate_tensors_with_dynamic_shape(tensors, name=...): # -> object | Operation | tuple[Any, ...] | list[Any]:
  r"""TODO: add doc.

  Args:
    tensors: A list of `Tensor` objects.
    name: A name for the operation (optional).

  Returns:
    A list of `Tensor` objects. Has the same type as `tensors`.
  """
  ...

TPUAnnotateTensorsWithDynamicShape = ...
def tpu_annotate_tensors_with_dynamic_shape_eager_fallback(tensors, name, ctx): # -> object:
  ...

def tpu_copy_with_dynamic_shape(tensors, unpadded_sizes: Annotated[List[Any], _atypes.Int32], name=...): # -> object | Operation | tuple[Any, ...] | list[Any]:
  r"""Op that copies host tensor to device with dynamic shape support.
For internal use only.

  Args:
    tensors: A list of `Tensor` objects.
    unpadded_sizes: A list of `Tensor` objects with type `int32`.
    name: A name for the operation (optional).

  Returns:
    A list of `Tensor` objects. Has the same type as `tensors`.
  """
  ...

TPUCopyWithDynamicShape = ...
def tpu_copy_with_dynamic_shape_eager_fallback(tensors, unpadded_sizes: Annotated[List[Any], _atypes.Int32], name, ctx): # -> object:
  ...

_XlaSparseCoreAdagradOutput = ...
def xla_sparse_core_adagrad(indices: Annotated[Any, _atypes.Int32], gradient: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], feature_width: int, name=...): # -> XlaSparseCoreAdagrad:
  r"""TODO: add doc.

  Args:
    indices: A `Tensor` of type `int32`.
    gradient: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    accumulator: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    feature_width: An `int`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (updated_embedding_table, updated_accumulator).

    updated_embedding_table: A `Tensor` of type `float32`.
    updated_accumulator: A `Tensor` of type `float32`.
  """
  ...

XlaSparseCoreAdagrad = ...
def xla_sparse_core_adagrad_eager_fallback(indices: Annotated[Any, _atypes.Int32], gradient: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], feature_width: int, name, ctx): # -> XlaSparseCoreAdagrad:
  ...

_XlaSparseCoreAdagradMomentumOutput = ...
def xla_sparse_core_adagrad_momentum(indices: Annotated[Any, _atypes.Int32], gradient: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], beta_1: Annotated[Any, _atypes.Float32], epsilon: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], momentum: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], feature_width: int, use_nesterov: bool, beta_2: float, exponent: float, name=...): # -> XlaSparseCoreAdagradMomentum:
  r"""TODO: add doc.

  Args:
    indices: A `Tensor` of type `int32`.
    gradient: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    beta_1: A `Tensor` of type `float32`.
    epsilon: A `Tensor` of type `float32`.
    accumulator: A `Tensor` of type `float32`.
    momentum: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    feature_width: An `int`.
    use_nesterov: A `bool`.
    beta_2: A `float`.
    exponent: A `float`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (updated_embedding_table, updated_accumulator, updated_momentum).

    updated_embedding_table: A `Tensor` of type `float32`.
    updated_accumulator: A `Tensor` of type `float32`.
    updated_momentum: A `Tensor` of type `float32`.
  """
  ...

XlaSparseCoreAdagradMomentum = ...
def xla_sparse_core_adagrad_momentum_eager_fallback(indices: Annotated[Any, _atypes.Int32], gradient: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], beta_1: Annotated[Any, _atypes.Float32], epsilon: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], momentum: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], feature_width: int, use_nesterov: bool, beta_2: float, exponent: float, name, ctx): # -> XlaSparseCoreAdagradMomentum:
  ...

_XlaSparseCoreAdamOutput = ...
def xla_sparse_core_adam(embedding_table: Annotated[Any, _atypes.Float32], indices: Annotated[Any, _atypes.Int32], gradient: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], momentum: Annotated[Any, _atypes.Float32], velocity: Annotated[Any, _atypes.Float32], beta_1: Annotated[Any, _atypes.Float32], beta_2: Annotated[Any, _atypes.Float32], epsilon: Annotated[Any, _atypes.Float32], feature_width: int, use_sum_inside_sqrt: bool, name=...): # -> XlaSparseCoreAdam:
  r"""TODO: add doc.

  Args:
    embedding_table: A `Tensor` of type `float32`.
    indices: A `Tensor` of type `int32`.
    gradient: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    momentum: A `Tensor` of type `float32`.
    velocity: A `Tensor` of type `float32`.
    beta_1: A `Tensor` of type `float32`.
    beta_2: A `Tensor` of type `float32`.
    epsilon: A `Tensor` of type `float32`.
    feature_width: An `int`.
    use_sum_inside_sqrt: A `bool`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (updated_embedding_table, updated_velocity, updated_momentum).

    updated_embedding_table: A `Tensor` of type `float32`.
    updated_velocity: A `Tensor` of type `float32`.
    updated_momentum: A `Tensor` of type `float32`.
  """
  ...

XlaSparseCoreAdam = ...
def xla_sparse_core_adam_eager_fallback(embedding_table: Annotated[Any, _atypes.Float32], indices: Annotated[Any, _atypes.Int32], gradient: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], momentum: Annotated[Any, _atypes.Float32], velocity: Annotated[Any, _atypes.Float32], beta_1: Annotated[Any, _atypes.Float32], beta_2: Annotated[Any, _atypes.Float32], epsilon: Annotated[Any, _atypes.Float32], feature_width: int, use_sum_inside_sqrt: bool, name, ctx): # -> XlaSparseCoreAdam:
  ...

_XlaSparseCoreFtrlOutput = ...
def xla_sparse_core_ftrl(embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], linear: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], indices: Annotated[Any, _atypes.Int32], gradient: Annotated[Any, _atypes.Float32], beta: Annotated[Any, _atypes.Float32], learning_rate_power: Annotated[Any, _atypes.Float32], l2_regularization_strength: Annotated[Any, _atypes.Float32], feature_width: int, multiply_linear_by_learning_rate: bool, l1_regularization_strength: float, name=...): # -> XlaSparseCoreFtrl:
  r"""TODO: add doc.

  Args:
    embedding_table: A `Tensor` of type `float32`.
    accumulator: A `Tensor` of type `float32`.
    linear: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    indices: A `Tensor` of type `int32`.
    gradient: A `Tensor` of type `float32`.
    beta: A `Tensor` of type `float32`.
    learning_rate_power: A `Tensor` of type `float32`.
    l2_regularization_strength: A `Tensor` of type `float32`.
    feature_width: An `int`.
    multiply_linear_by_learning_rate: A `bool`.
    l1_regularization_strength: A `float`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (updated_embedding_table, updated_accumulator, updated_linear).

    updated_embedding_table: A `Tensor` of type `float32`.
    updated_accumulator: A `Tensor` of type `float32`.
    updated_linear: A `Tensor` of type `float32`.
  """
  ...

XlaSparseCoreFtrl = ...
def xla_sparse_core_ftrl_eager_fallback(embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], linear: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], indices: Annotated[Any, _atypes.Int32], gradient: Annotated[Any, _atypes.Float32], beta: Annotated[Any, _atypes.Float32], learning_rate_power: Annotated[Any, _atypes.Float32], l2_regularization_strength: Annotated[Any, _atypes.Float32], feature_width: int, multiply_linear_by_learning_rate: bool, l1_regularization_strength: float, name, ctx): # -> XlaSparseCoreFtrl:
  ...

def xla_sparse_core_sgd(indices: Annotated[Any, _atypes.Int32], gradient: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], feature_width: int, name=...) -> Annotated[Any, _atypes.Float32]:
  r"""TODO: add doc.

  Args:
    indices: A `Tensor` of type `int32`.
    gradient: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    feature_width: An `int`.
    name: A name for the operation (optional).

  Returns:
    A `Tensor` of type `float32`.
  """
  ...

XlaSparseCoreSgd = ...
def xla_sparse_core_sgd_eager_fallback(indices: Annotated[Any, _atypes.Int32], gradient: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], feature_width: int, name, ctx) -> Annotated[Any, _atypes.Float32]:
  ...

_XlaSparseDenseMatmulOutput = ...
def xla_sparse_dense_matmul(row_ids: Annotated[Any, _atypes.Int32], col_ids: Annotated[Any, _atypes.UInt32], values: Annotated[Any, _atypes.Float32], offsets: Annotated[Any, _atypes.UInt32], embedding_table: Annotated[Any, _atypes.Float32], max_ids_per_partition: int, max_unique_ids_per_partition: int, input_size: int, name=...): # -> XlaSparseDenseMatmul:
  r"""TODO: add doc.

  Args:
    row_ids: A `Tensor` of type `int32`.
    col_ids: A `Tensor` of type `uint32`.
    values: A `Tensor` of type `float32`.
    offsets: A `Tensor` of type `uint32`.
    embedding_table: A `Tensor` of type `float32`.
    max_ids_per_partition: An `int` that is `>= 0`.
    max_unique_ids_per_partition: An `int` that is `>= 0`.
    input_size: An `int` that is `>= 0`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (activations, row_pointers, sorted_embedding_ids, sorted_sample_ids, sorted_gains).

    activations: A `Tensor` of type `float32`.
    row_pointers: A `Tensor` of type `int32`.
    sorted_embedding_ids: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
  """
  ...

XlaSparseDenseMatmul = ...
def xla_sparse_dense_matmul_eager_fallback(row_ids: Annotated[Any, _atypes.Int32], col_ids: Annotated[Any, _atypes.UInt32], values: Annotated[Any, _atypes.Float32], offsets: Annotated[Any, _atypes.UInt32], embedding_table: Annotated[Any, _atypes.Float32], max_ids_per_partition: int, max_unique_ids_per_partition: int, input_size: int, name, ctx): # -> XlaSparseDenseMatmul:
  ...

_XlaSparseDenseMatmulGradWithAdagradAndCsrInputOutput = ...
def xla_sparse_dense_matmul_grad_with_adagrad_and_csr_input(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], table_name: str, clip_weight_min: float = ..., clip_weight_max: float = ..., name=...): # -> XlaSparseDenseMatmulGradWithAdagradAndCsrInput:
  r"""TODO: add doc.

  Args:
    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    activation_gradients: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    accumulator: A `Tensor` of type `float32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
    table_name: A `string`.
    clip_weight_min: An optional `float`. Defaults to `float('-inf')`.
    clip_weight_max: An optional `float`. Defaults to `float('inf')`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (updated_embedding_table, updated_accumulator).

    updated_embedding_table: A `Tensor` of type `float32`.
    updated_accumulator: A `Tensor` of type `float32`.
  """
  ...

XlaSparseDenseMatmulGradWithAdagradAndCsrInput = ...
def xla_sparse_dense_matmul_grad_with_adagrad_and_csr_input_eager_fallback(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], table_name: str, clip_weight_min: float, clip_weight_max: float, name, ctx): # -> XlaSparseDenseMatmulGradWithAdagradAndCsrInput:
  ...

_XlaSparseDenseMatmulGradWithAdagradAndStaticBufferSizeOutput = ...
def xla_sparse_dense_matmul_grad_with_adagrad_and_static_buffer_size(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, clip_weight_min: float = ..., clip_weight_max: float = ..., name=...): # -> XlaSparseDenseMatmulGradWithAdagradAndStaticBufferSize:
  r"""TODO: add doc.

  Args:
    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    activation_gradients: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    accumulator: A `Tensor` of type `float32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
    max_ids_per_sparse_core: An `int` that is `>= 1`.
    max_unique_ids_per_sparse_core: An `int` that is `>= 1`.
    table_name: A `string`.
    clip_weight_min: An optional `float`. Defaults to `float('-inf')`.
    clip_weight_max: An optional `float`. Defaults to `float('inf')`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (updated_embedding_table, updated_accumulator).

    updated_embedding_table: A `Tensor` of type `float32`.
    updated_accumulator: A `Tensor` of type `float32`.
  """
  ...

XlaSparseDenseMatmulGradWithAdagradAndStaticBufferSize = ...
def xla_sparse_dense_matmul_grad_with_adagrad_and_static_buffer_size_eager_fallback(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, clip_weight_min: float, clip_weight_max: float, name, ctx): # -> XlaSparseDenseMatmulGradWithAdagradAndStaticBufferSize:
  ...

_XlaSparseDenseMatmulGradWithAdagradMomentumAndCsrInputOutput = ...
def xla_sparse_dense_matmul_grad_with_adagrad_momentum_and_csr_input(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], momenta: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], use_nesterov: bool, exponent: float, beta1: float, beta2: float, epsilon: float, table_name: str, clip_weight_min: float = ..., clip_weight_max: float = ..., name=...): # -> XlaSparseDenseMatmulGradWithAdagradMomentumAndCsrInput:
  r"""TODO: add doc.

  Args:
    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    activation_gradients: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    accumulator: A `Tensor` of type `float32`.
    momenta: A `Tensor` of type `float32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
    use_nesterov: A `bool`.
    exponent: A `float`.
    beta1: A `float`.
    beta2: A `float`.
    epsilon: A `float`.
    table_name: A `string`.
    clip_weight_min: An optional `float`. Defaults to `float('-inf')`.
    clip_weight_max: An optional `float`. Defaults to `float('inf')`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (updated_embedding_table, updated_accumulator, updated_momenta).

    updated_embedding_table: A `Tensor` of type `float32`.
    updated_accumulator: A `Tensor` of type `float32`.
    updated_momenta: A `Tensor` of type `float32`.
  """
  ...

XlaSparseDenseMatmulGradWithAdagradMomentumAndCsrInput = ...
def xla_sparse_dense_matmul_grad_with_adagrad_momentum_and_csr_input_eager_fallback(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], momenta: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], use_nesterov: bool, exponent: float, beta1: float, beta2: float, epsilon: float, table_name: str, clip_weight_min: float, clip_weight_max: float, name, ctx): # -> XlaSparseDenseMatmulGradWithAdagradMomentumAndCsrInput:
  ...

_XlaSparseDenseMatmulGradWithAdagradMomentumAndStaticBufferSizeOutput = ...
def xla_sparse_dense_matmul_grad_with_adagrad_momentum_and_static_buffer_size(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], momenta: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], use_nesterov: bool, exponent: float, beta1: float, beta2: float, epsilon: float, max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, clip_weight_min: float = ..., clip_weight_max: float = ..., name=...): # -> XlaSparseDenseMatmulGradWithAdagradMomentumAndStaticBufferSize:
  r"""TODO: add doc.

  Args:
    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    activation_gradients: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    accumulator: A `Tensor` of type `float32`.
    momenta: A `Tensor` of type `float32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
    use_nesterov: A `bool`.
    exponent: A `float`.
    beta1: A `float`.
    beta2: A `float`.
    epsilon: A `float`.
    max_ids_per_sparse_core: An `int` that is `>= 1`.
    max_unique_ids_per_sparse_core: An `int` that is `>= 1`.
    table_name: A `string`.
    clip_weight_min: An optional `float`. Defaults to `float('-inf')`.
    clip_weight_max: An optional `float`. Defaults to `float('inf')`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (updated_embedding_table, updated_accumulator, updated_momenta).

    updated_embedding_table: A `Tensor` of type `float32`.
    updated_accumulator: A `Tensor` of type `float32`.
    updated_momenta: A `Tensor` of type `float32`.
  """
  ...

XlaSparseDenseMatmulGradWithAdagradMomentumAndStaticBufferSize = ...
def xla_sparse_dense_matmul_grad_with_adagrad_momentum_and_static_buffer_size_eager_fallback(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], momenta: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], use_nesterov: bool, exponent: float, beta1: float, beta2: float, epsilon: float, max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, clip_weight_min: float, clip_weight_max: float, name, ctx): # -> XlaSparseDenseMatmulGradWithAdagradMomentumAndStaticBufferSize:
  ...

_XlaSparseDenseMatmulGradWithAdamAndCsrInputOutput = ...
def xla_sparse_dense_matmul_grad_with_adam_and_csr_input(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], momenta: Annotated[Any, _atypes.Float32], velocity: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], use_sum_inside_sqrt: bool, beta1: float, beta2: float, epsilon: float, table_name: str, clip_weight_min: float = ..., clip_weight_max: float = ..., name=...): # -> XlaSparseDenseMatmulGradWithAdamAndCsrInput:
  r"""TODO: add doc.

  Args:
    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    activation_gradients: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    momenta: A `Tensor` of type `float32`.
    velocity: A `Tensor` of type `float32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
    use_sum_inside_sqrt: A `bool`.
    beta1: A `float`.
    beta2: A `float`.
    epsilon: A `float`.
    table_name: A `string`.
    clip_weight_min: An optional `float`. Defaults to `float('-inf')`.
    clip_weight_max: An optional `float`. Defaults to `float('inf')`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (updated_embedding_table, updated_momenta, updated_velocity).

    updated_embedding_table: A `Tensor` of type `float32`.
    updated_momenta: A `Tensor` of type `float32`.
    updated_velocity: A `Tensor` of type `float32`.
  """
  ...

XlaSparseDenseMatmulGradWithAdamAndCsrInput = ...
def xla_sparse_dense_matmul_grad_with_adam_and_csr_input_eager_fallback(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], momenta: Annotated[Any, _atypes.Float32], velocity: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], use_sum_inside_sqrt: bool, beta1: float, beta2: float, epsilon: float, table_name: str, clip_weight_min: float, clip_weight_max: float, name, ctx): # -> XlaSparseDenseMatmulGradWithAdamAndCsrInput:
  ...

_XlaSparseDenseMatmulGradWithAdamAndStaticBufferSizeOutput = ...
def xla_sparse_dense_matmul_grad_with_adam_and_static_buffer_size(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], momenta: Annotated[Any, _atypes.Float32], velocity: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], use_sum_inside_sqrt: bool, beta1: float, beta2: float, epsilon: float, max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, clip_weight_min: float = ..., clip_weight_max: float = ..., name=...): # -> XlaSparseDenseMatmulGradWithAdamAndStaticBufferSize:
  r"""TODO: add doc.

  Args:
    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    activation_gradients: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    momenta: A `Tensor` of type `float32`.
    velocity: A `Tensor` of type `float32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
    use_sum_inside_sqrt: A `bool`.
    beta1: A `float`.
    beta2: A `float`.
    epsilon: A `float`.
    max_ids_per_sparse_core: An `int` that is `>= 1`.
    max_unique_ids_per_sparse_core: An `int` that is `>= 1`.
    table_name: A `string`.
    clip_weight_min: An optional `float`. Defaults to `float('-inf')`.
    clip_weight_max: An optional `float`. Defaults to `float('inf')`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (updated_embedding_table, updated_momenta, updated_velocity).

    updated_embedding_table: A `Tensor` of type `float32`.
    updated_momenta: A `Tensor` of type `float32`.
    updated_velocity: A `Tensor` of type `float32`.
  """
  ...

XlaSparseDenseMatmulGradWithAdamAndStaticBufferSize = ...
def xla_sparse_dense_matmul_grad_with_adam_and_static_buffer_size_eager_fallback(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], momenta: Annotated[Any, _atypes.Float32], velocity: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], use_sum_inside_sqrt: bool, beta1: float, beta2: float, epsilon: float, max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, clip_weight_min: float, clip_weight_max: float, name, ctx): # -> XlaSparseDenseMatmulGradWithAdamAndStaticBufferSize:
  ...

def xla_sparse_dense_matmul_grad_with_csr_input(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], tables: Annotated[List[Any], _atypes.Float32], hyperparameters: Annotated[List[Any], _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], custom_computation, table_name: str, name=...): # -> object | tuple[Any, ...] | list[Any]:
  r"""TODO: add doc.

  Args:
    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    activation_gradients: A `Tensor` of type `float32`.
    tables: A list of at least 1 `Tensor` objects with type `float32`.
    hyperparameters: A list of at least 1 `Tensor` objects with type `float32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
    custom_computation: A function decorated with @Defun.
    table_name: A `string`.
    name: A name for the operation (optional).

  Returns:
    A list with the same length as `tables` of `Tensor` objects with type `float32`.
  """
  ...

XlaSparseDenseMatmulGradWithCsrInput = ...
def xla_sparse_dense_matmul_grad_with_csr_input_eager_fallback(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], tables: Annotated[List[Any], _atypes.Float32], hyperparameters: Annotated[List[Any], _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], custom_computation, table_name: str, name, ctx): # -> object:
  ...

_XlaSparseDenseMatmulGradWithFtrlAndCsrInputOutput = ...
def xla_sparse_dense_matmul_grad_with_ftrl_and_csr_input(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], linear: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], multiply_linear_by_learning_rate: bool, beta: float, learning_rate_power: float, l1_regularization_strength: float, l2_regularization_strength: float, table_name: str, clip_weight_min: float = ..., clip_weight_max: float = ..., name=...): # -> XlaSparseDenseMatmulGradWithFtrlAndCsrInput:
  r"""TODO: add doc.

  Args:
    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    activation_gradients: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    accumulator: A `Tensor` of type `float32`.
    linear: A `Tensor` of type `float32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
    multiply_linear_by_learning_rate: A `bool`.
    beta: A `float`.
    learning_rate_power: A `float`.
    l1_regularization_strength: A `float`.
    l2_regularization_strength: A `float`.
    table_name: A `string`.
    clip_weight_min: An optional `float`. Defaults to `float('-inf')`.
    clip_weight_max: An optional `float`. Defaults to `float('inf')`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (updated_embedding_table, updated_accumulator, updated_linear).

    updated_embedding_table: A `Tensor` of type `float32`.
    updated_accumulator: A `Tensor` of type `float32`.
    updated_linear: A `Tensor` of type `float32`.
  """
  ...

XlaSparseDenseMatmulGradWithFtrlAndCsrInput = ...
def xla_sparse_dense_matmul_grad_with_ftrl_and_csr_input_eager_fallback(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], linear: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], multiply_linear_by_learning_rate: bool, beta: float, learning_rate_power: float, l1_regularization_strength: float, l2_regularization_strength: float, table_name: str, clip_weight_min: float, clip_weight_max: float, name, ctx): # -> XlaSparseDenseMatmulGradWithFtrlAndCsrInput:
  ...

_XlaSparseDenseMatmulGradWithFtrlAndStaticBufferSizeOutput = ...
def xla_sparse_dense_matmul_grad_with_ftrl_and_static_buffer_size(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], linear: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], multiply_linear_by_learning_rate: bool, beta: float, learning_rate_power: float, l1_regularization_strength: float, l2_regularization_strength: float, max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, clip_weight_min: float = ..., clip_weight_max: float = ..., name=...): # -> XlaSparseDenseMatmulGradWithFtrlAndStaticBufferSize:
  r"""TODO: add doc.

  Args:
    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    activation_gradients: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    accumulator: A `Tensor` of type `float32`.
    linear: A `Tensor` of type `float32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
    multiply_linear_by_learning_rate: A `bool`.
    beta: A `float`.
    learning_rate_power: A `float`.
    l1_regularization_strength: A `float`.
    l2_regularization_strength: A `float`.
    max_ids_per_sparse_core: An `int` that is `>= 1`.
    max_unique_ids_per_sparse_core: An `int` that is `>= 1`.
    table_name: A `string`.
    clip_weight_min: An optional `float`. Defaults to `float('-inf')`.
    clip_weight_max: An optional `float`. Defaults to `float('inf')`.
    name: A name for the operation (optional).

  Returns:
    A tuple of `Tensor` objects (updated_embedding_table, updated_accumulator, updated_linear).

    updated_embedding_table: A `Tensor` of type `float32`.
    updated_accumulator: A `Tensor` of type `float32`.
    updated_linear: A `Tensor` of type `float32`.
  """
  ...

XlaSparseDenseMatmulGradWithFtrlAndStaticBufferSize = ...
def xla_sparse_dense_matmul_grad_with_ftrl_and_static_buffer_size_eager_fallback(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], accumulator: Annotated[Any, _atypes.Float32], linear: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], multiply_linear_by_learning_rate: bool, beta: float, learning_rate_power: float, l1_regularization_strength: float, l2_regularization_strength: float, max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, clip_weight_min: float, clip_weight_max: float, name, ctx): # -> XlaSparseDenseMatmulGradWithFtrlAndStaticBufferSize:
  ...

def xla_sparse_dense_matmul_grad_with_sgd_and_csr_input(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], table_name: str, clip_weight_min: float = ..., clip_weight_max: float = ..., name=...) -> Annotated[Any, _atypes.Float32]:
  r"""TODO: add doc.

  Args:
    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    activation_gradients: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
    table_name: A `string`.
    clip_weight_min: An optional `float`. Defaults to `float('-inf')`.
    clip_weight_max: An optional `float`. Defaults to `float('inf')`.
    name: A name for the operation (optional).

  Returns:
    A `Tensor` of type `float32`.
  """
  ...

XlaSparseDenseMatmulGradWithSgdAndCsrInput = ...
def xla_sparse_dense_matmul_grad_with_sgd_and_csr_input_eager_fallback(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], table_name: str, clip_weight_min: float, clip_weight_max: float, name, ctx) -> Annotated[Any, _atypes.Float32]:
  ...

def xla_sparse_dense_matmul_grad_with_sgd_and_static_buffer_size(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, clip_weight_min: float = ..., clip_weight_max: float = ..., name=...) -> Annotated[Any, _atypes.Float32]:
  r"""TODO: add doc.

  Args:
    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    activation_gradients: A `Tensor` of type `float32`.
    learning_rate: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
    max_ids_per_sparse_core: An `int` that is `>= 1`.
    max_unique_ids_per_sparse_core: An `int` that is `>= 1`.
    table_name: A `string`.
    clip_weight_min: An optional `float`. Defaults to `float('-inf')`.
    clip_weight_max: An optional `float`. Defaults to `float('inf')`.
    name: A name for the operation (optional).

  Returns:
    A `Tensor` of type `float32`.
  """
  ...

XlaSparseDenseMatmulGradWithSgdAndStaticBufferSize = ...
def xla_sparse_dense_matmul_grad_with_sgd_and_static_buffer_size_eager_fallback(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], activation_gradients: Annotated[Any, _atypes.Float32], learning_rate: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, clip_weight_min: float, clip_weight_max: float, name, ctx) -> Annotated[Any, _atypes.Float32]:
  ...

def xla_sparse_dense_matmul_with_csr_input(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], input_size: int, quantization_config_low: float, quantization_config_high: float, quantization_config_num_buckets: int, table_name: str, name=...) -> Annotated[Any, _atypes.Float32]:
  r"""TODO: add doc.

  Args:
    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
    input_size: An `int` that is `>= 0`.
    quantization_config_low: A `float`.
    quantization_config_high: A `float`.
    quantization_config_num_buckets: An `int` that is `>= 0`.
    table_name: A `string`.
    name: A name for the operation (optional).

  Returns:
    A `Tensor` of type `float32`.
  """
  ...

XlaSparseDenseMatmulWithCsrInput = ...
def xla_sparse_dense_matmul_with_csr_input_eager_fallback(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], input_size: int, quantization_config_low: float, quantization_config_high: float, quantization_config_num_buckets: int, table_name: str, name, ctx) -> Annotated[Any, _atypes.Float32]:
  ...

def xla_sparse_dense_matmul_with_static_buffer_size(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], input_size: int, quantization_config_low: float, quantization_config_high: float, quantization_config_num_buckets: int, max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, name=...) -> Annotated[Any, _atypes.Float32]:
  r"""TODO: add doc.

  Args:
    row_pointers: A `Tensor` of type `int32`.
    sorted_sample_ids: A `Tensor` of type `int32`.
    sorted_token_ids: A `Tensor` of type `int32`.
    sorted_gains: A `Tensor` of type `float32`.
    embedding_table: A `Tensor` of type `float32`.
    num_minibatches_per_physical_sparse_core: A `Tensor` of type `int32`.
    input_size: An `int` that is `>= 0`.
    quantization_config_low: A `float`.
    quantization_config_high: A `float`.
    quantization_config_num_buckets: An `int` that is `>= 0`.
    max_ids_per_sparse_core: An `int` that is `>= 1`.
    max_unique_ids_per_sparse_core: An `int` that is `>= 1`.
    table_name: A `string`.
    name: A name for the operation (optional).

  Returns:
    A `Tensor` of type `float32`.
  """
  ...

XlaSparseDenseMatmulWithStaticBufferSize = ...
def xla_sparse_dense_matmul_with_static_buffer_size_eager_fallback(row_pointers: Annotated[Any, _atypes.Int32], sorted_sample_ids: Annotated[Any, _atypes.Int32], sorted_token_ids: Annotated[Any, _atypes.Int32], sorted_gains: Annotated[Any, _atypes.Float32], embedding_table: Annotated[Any, _atypes.Float32], num_minibatches_per_physical_sparse_core: Annotated[Any, _atypes.Int32], input_size: int, quantization_config_low: float, quantization_config_high: float, quantization_config_num_buckets: int, max_ids_per_sparse_core: int, max_unique_ids_per_sparse_core: int, table_name: str, name, ctx) -> Annotated[Any, _atypes.Float32]:
  ...

