"""
This type stub file was generated by pyright.
"""

import dataclasses
from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union
from tensorflow.core.tpu.kernels import sparse_core_layout_pb2
from tensorflow.python.checkpoint import saveable_compat
from tensorflow.python.distribute import distribute_lib, tpu_values, values
from tensorflow.python.framework import dtypes, ops, tensor, tensor_shape
from tensorflow.python.ops import control_flow_ops, variables as tf_variables
from tensorflow.python.tpu import tpu_embedding_base, tpu_embedding_v2_utils
from tensorflow.python.training.saving import saveable_object
from tensorflow.python.util.tf_export import tf_export

"""Mid level API for TPU Embeddings With V2 Embedding Accelerator."""
_PIPELINE_ATTRIBUTE = ...
_PIPELINE_MODE_FORWARD = ...
_PIPELINE_MODE_BACKWARD = ...
TableConfig = tpu_embedding_v2_utils.TableConfig
FeatureConfig = tpu_embedding_v2_utils.TableConfig
QuantizationConfig = tpu_embedding_v2_utils.QuantizationConfig
@tf_export("tpu.experimental.embedding.SparseCoreEmbeddingConfig")
@dataclasses.dataclass(frozen=True)
class SparseCoreEmbeddingConfig:
  """Config for sparsecore embedding."""
  disable_table_stacking: bool = ...
  max_ids_per_chip_per_sample: int = ...
  max_ids_per_table: Optional[Dict[str, int]] = ...
  max_unique_ids_per_table: Optional[Dict[str, int]] = ...
  allow_id_dropping: bool = ...
  initialize_tables_on_host: bool = ...
  enable_fast_table_initialization: bool = ...


class EmbeddingPipeliningContext(control_flow_ops.ControlFlowContext):
  """Sets the _embedding_pipelining attribute on all ops created in the scope."""
  def __init__(self, mode: str, enable: bool) -> None:
    ...
  
  def to_control_flow_context_def(self, context_def: Any, export_scope: Any = ...): # -> None:
    ...
  
  def AddOp(self, op: ops.Operation): # -> None:
    ...
  


class TPUEmbeddingShardedSaveable(saveable_object.SaveableObject):
  """Defines how to save and restore a shard of TPUEmbedding sharded variable."""
  def __init__(self, variable: tf_variables.Variable, shard_id: int, num_shards: int, shard_dim: int, name: str) -> None:
    """Init TPUEmbeddingShardedSaveable."""
    ...
  
  def restore(self, restored_tensors: List[tensor.Tensor], restored_shapes: List[tensor_shape.TensorShape]) -> Any:
    ...
  


@dataclasses.dataclass
class TableStacking:
  """Information about how we stack tables."""
  stacked_table_to_tables: Dict[str, TableConfig] = ...
  quantization_configs: Dict[str, QuantizationConfig] = ...
  table_name_to_table: Dict[str, TableConfig] = ...
  table_to_padding_rows: Dict[str, int] = ...
  table_to_padding_columns: Dict[str, int] = ...
  table_to_sample_count: Dict[str, int] = ...
  table_to_layout: Dict[str, sparse_core_layout_pb2.SparseCoreTableLayout] = ...
  table_to_stacked_table_offset: Dict[str, Tuple[str, int, int]] = ...
  feature_to_sample_offset: Dict[str, int] = ...


@saveable_compat.legacy_saveable_name("")
class TPUEmbeddingShardedVariable(tpu_values.TPUVariableMixin, values.DistributedVariable):
  """A ShardedVariable class for Embedding tables on TPU."""
  @property
  def shard_dim(self) -> int:
    ...
  
  @property
  def shape(self) -> tensor_shape.TensorShape:
    """Returns the shape of the embedding variable for the current context."""
    ...
  
  def read_value(self) -> Any:
    ...
  
  def assign(self, value: Any, use_locking: bool = ..., name: Optional[Any] = ..., read_value: bool = ...) -> Any:
    ...
  
  def assign_on_device(self, device, value): # -> None:
    ...
  
  def read_from_device(self, device): # -> Any:
    ...
  


PartitionedCsrFormatTensor = ...
@tf_export("tpu.experimental.embedding.TPUEmbeddingV2")
class TPUEmbeddingV2(tpu_embedding_base.TPUEmbeddingBase):
  """The TPUEmbedding mid level API running on TPU with sparse core accelerator."""
  DEFAULT_MAX_IDS_PER_TABLE = ...
  DEFAULT_MAX_UNIQUE_IDS_PER_TABLE = ...
  def __init__(self, feature_config: Union[tpu_embedding_v2_utils.FeatureConfig, Iterable], optimizer: Optional[tpu_embedding_v2_utils._Optimizer] = ..., pipeline_execution_with_tensor_core: bool = ..., sparse_core_embedding_config: Optional[SparseCoreEmbeddingConfig] = ...) -> None:
    """Creates the TPUEmbeddingV2 mid level API object.

    Args:
      feature_config: A nested structure of
        `tf.tpu.experimental.embedding.FeatureConfig` configs.
      optimizer: An instance of one of `tf.tpu.experimental.embedding.SGD`,
        `tf.tpu.experimental.embedding.Adagrad` or
        `tf.tpu.experimental.embedding.Adam`. When not created under TPUStrategy
        may be set to None to avoid the creation of the optimizer slot
        variables, useful for optimizing memory consumption when exporting the
        model for serving where slot variables aren't needed.
      pipeline_execution_with_tensor_core: If True, the TPU embedding
        computations will overlap with the TensorCore computations (and hence
        will be one step old). Set to True for improved performance.
      sparse_core_embedding_config: Configs for sparse core embedding including
        settings for table stacking, input feature static buffer size etc.

    Raises:
      ValueError: If optimizer is not one of tf.tpu.experimental.embedding.(SGD,
      Adam or Adagrad) or None when created under a TPUStrategy.
      RuntimeError: If not created under TPUStrategy.
    """
    ...
  
  @property
  def embedding_tables(self) -> Dict[tpu_embedding_v2_utils.TableConfig, tf_variables.Variable]:
    """Returns a dict of embedding tables, keyed by `TableConfig`."""
    ...
  
  @property
  def embedding_table_shards(self) -> Dict[tpu_embedding_v2_utils.TableConfig, List[tf_variables.Variable]]:
    """Returns a dict of embedding tables, keyed by `TableConfig`."""
    ...
  
  @property
  def embedding_layouts(self) -> Dict[str, sparse_core_layout_pb2.SparseCoreTableLayout]:
    """Returns how the tables are laid out in the variables.

    The SparseCoreTableLayout describes how a table is stored in its internal
    state. You need this only if you need to pull apart the internal state.
    """
    ...
  
  @property
  def variables(self) -> Dict[tpu_embedding_v2_utils.TableConfig, Dict[str, tf_variables.Variable]]:
    """Returns a dict of variables, keyed by `TableConfig`, then by slot name."""
    ...
  
  def build(self): # -> None:
    """Create variables and slots variables for TPU embeddings."""
    ...
  
  def apply_gradients(self, gradients: Any, preserved_outputs: Dict[str, PartitionedCsrFormatTensor]): # -> None:
    """Applies the gradient update to the embedding tables.

    If a gradient of `None` is passed in any position of the nested structure,
    then a gradient update with a zero gradient is applied for that feature.
    For optimizers like SGD or Adagrad, this is the same as applying no update
    at all. For lazy Adam and other sparsely applied optimizers with decay,
    ensure you understand the effect of applying a zero gradient.

    Args:
      gradients: A nested structure of gradients, with structure matching the
        `feature_config` passed to this object.
      preserved_outputs: A dicts of PartitionedCsrFormatTensor, coming from the
        second output of the embedding lookup call.

    Raises:
      RuntimeError: if not built.
      ValueError: If a non-`tf.Tensor` non-`None` gradient is passed in, or a
        `tf.Tensor` of the incorrect shape is passed in. Also if
        the size of any sequence in `gradients` does not match corresponding
        sequence in `feature_config`.
      TypeError: If the type of any sequence in `gradients` does not match
        corresponding sequence in `feature_config`.
    """
    ...
  
  def __call__(self, features: Any, weights: Optional[Any] = ...) -> Tuple[Any, Dict[str, PartitionedCsrFormatTensor]]:
    """Call the mid level api to do embedding lookup."""
    ...
  
  @classmethod
  def compute_sparse_core_stats(cls, features: Any, feature_config: Union[FeatureConfig, Iterable], num_tpu_chips: int, num_sc_per_chip: int = ..., optimizer: Optional[tpu_embedding_v2_utils._Optimizer] = ..., sparse_core_embedding_config: Optional[SparseCoreEmbeddingConfig] = ...) -> Tuple[Any, Any]:
    """Computes the max_ids/unique ids settings from the input features."""
    ...
  
  def enqueue(self, features: Any, weights: Optional[Any] = ..., device: Optional[str] = ...) -> Any:
    """Preprocessing the features on host."""
    ...
  
  def dequeue(self, partitioned_tensors: Tuple[Dict[str, PartitionedCsrFormatTensor], int, int]) -> Tuple[Any, Dict[str, PartitionedCsrFormatTensor]]:
    """Perform embedding lookup."""
    ...
  
  def embedding_lookup(self, features: Any, weights: Optional[Any] = ...) -> Tuple[Any, Dict[str, PartitionedCsrFormatTensor]]:
    """Perform embedding lookup on the input feature.

    Args:
      features: A nested structure of `tf.Tensor`s, `tf.SparseTensor`s or
        `tf.RaggedTensor`s, with the same structure as `feature_config`. Inputs
        will be downcast to `tf.int32`. Only one type out of `tf.SparseTensor`
        or `tf.RaggedTensor` is supported per call.
      weights: If not `None`, a nested structure of `tf.Tensor`s,
        `tf.SparseTensor`s or `tf.RaggedTensor`s, matching the above, except
        that the tensors should be of float type (and they will be downcast to
        `tf.float32`). For `tf.SparseTensor`s we assume the `indices` are the
        same for the parallel entries from `features` and similarly for
        `tf.RaggedTensor`s we assume the row_splits are the same.

    Raises:
      ValueError: If the input feature is not one of the Tensor, SparseTensor or
        RaggedTensor type.
      TypeError: If the type of any sequence in `features` does not match
        corresponding sequence in `feature_config`. Similarly for `weights`, if
        not `None`.

    Returns:
      packed_activations: Embedding lookup results packed as the same sequence
        of the input feature.
      packed_output: A dict of PartitionedCsrFormatTensors.
    """
    ...
  


def extract_variable_info(kwargs: Any) -> Tuple[str, Tuple[int, ...], dtypes.DType, Callable[[], Any], Optional[int]]:
  """Extracts the variable creation attributes from the kwargs.

  Args:
    kwargs: a dict of keyword arguments that were passed to a variable creator
      scope.

  Returns:
    A tuple of variable name, shape, dtype, initialization function,
    restore_uid.
  """
  ...

def is_checkpoint_initial_value(initial_value: Any) -> bool:
  """Whether the initial value is from checkpoint."""
  ...

def make_sharded_variable_creator(strategy: distribute_lib.Strategy) -> Callable[..., Any]:
  """Create a variable creator which shards across all the tpu device.

  Args:
    strategy: a TPUStrategy object.

  Returns:
    The sharded variable creator.
  """
  ...

