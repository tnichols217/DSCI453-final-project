"""
This type stub file was generated by pyright.
"""

import contextlib
from tensorflow.python.framework import ops, tensor
from tensorflow.python.ops import lookup_ops, resource_variable_ops
from tensorflow.python.types import core

"""Various classes representing distributed values for PS."""
TRACKABLE_RESOURCE_METHODS = ...
class AggregatingVariable(resource_variable_ops.BaseResourceVariable, core.Tensor):
  """A wrapper around a variable that aggregates updates across replicas."""
  def __init__(self, strategy, v, aggregation) -> None:
    ...
  
  def __deepcopy__(self, memo): # -> Self:
    """Perform a deepcopy of the `AggregatingVariable`.

    Unlike the deepcopy of a regular tf.Variable, this keeps the original
    strategy and devices of the `AggregatingVariable`.  To avoid confusion
    with the behavior of deepcopy on a regular `Variable` (which does
    copy into new devices), we only allow a deepcopy of a `AggregatingVariable`
    within its originating strategy scope.

    Args:
      memo: The memoization object for `deepcopy`.

    Returns:
      A deep copy of the current `AggregatingVariable`.

    Raises:
      RuntimeError: If trying to deepcopy into a different strategy.
    """
    ...
  
  def get(self): # -> Any:
    ...
  
  @property
  def distribute_strategy(self): # -> Any:
    ...
  
  def __getattr__(self, name): # -> Any:
    ...
  
  def assign_sub(self, *args, **kwargs): # -> Any:
    ...
  
  def assign_add(self, *args, **kwargs): # -> Any:
    ...
  
  def assign(self, *args, **kwargs): # -> Any:
    ...
  
  @property
  def initializer(self):
    ...
  
  def initialized_value(self):
    ...
  
  @property
  def initial_value(self):
    ...
  
  @property
  def op(self) -> ops.Operation:
    ...
  
  def value(self):
    ...
  
  def read_value(self):
    ...
  
  def sparse_read(self, indices, name=...):
    ...
  
  def eval(self, session=...):
    ...
  
  @property
  def graph(self):
    ...
  
  @property
  def device(self):
    ...
  
  @property
  def shape(self):
    ...
  
  @property
  def aggregation(self): # -> Any:
    ...
  
  @property
  def synchronization(self):
    ...
  
  @property
  def name(self):
    ...
  
  @property
  def trainable(self):
    ...
  
  @property
  def dtype(self):
    ...
  
  def __add__(self, o):
    ...
  
  def __radd__(self, o):
    ...
  
  def __sub__(self, o):
    ...
  
  def __rsub__(self, o):
    ...
  
  def __mul__(self, o):
    ...
  
  def __rmul__(self, o):
    ...
  
  def __truediv__(self, o):
    ...
  
  def __rtruediv__(self, o):
    ...
  
  def __floordiv__(self, o):
    ...
  
  def __rfloordiv__(self, o):
    ...
  
  def __mod__(self, o):
    ...
  
  def __rmod__(self, o):
    ...
  
  def __lt__(self, o) -> bool:
    ...
  
  def __le__(self, o) -> bool:
    ...
  
  def __gt__(self, o) -> bool:
    ...
  
  def __ge__(self, o) -> bool:
    ...
  
  def __and__(self, o):
    ...
  
  def __rand__(self, o):
    ...
  
  def __or__(self, o):
    ...
  
  def __ror__(self, o):
    ...
  
  def __xor__(self, o):
    ...
  
  def __rxor__(self, o):
    ...
  
  def __getitem__(self, o):
    ...
  
  def __pow__(self, o, modulo=...):
    ...
  
  def __rpow__(self, o):
    ...
  
  def __invert__(self):
    ...
  
  def __neg__(self):
    ...
  
  def __abs__(self):
    ...
  
  def __div__(self, o): # -> _NotImplementedType:
    ...
  
  def __rdiv__(self, o): # -> _NotImplementedType:
    ...
  
  def __matmul__(self, o): # -> _NotImplementedType:
    ...
  
  def __rmatmul__(self, o): # -> _NotImplementedType:
    ...
  
  def __str__(self) -> str:
    ...
  
  def __repr__(self): # -> str:
    ...
  


class CachingVariable(resource_variable_ops.BaseResourceVariable, core.Tensor):
  """A wrapper around a variable that caches read value locally."""
  def __init__(self, v) -> None:
    ...
  
  def get(self): # -> Any:
    ...
  
  def __getattr__(self, name): # -> Any:
    ...
  
  def read_value(self): # -> defaultdict[Any, Any] | Any | list[Any] | object | None:
    ...
  
  def sparse_read(self, indices, name=...):
    ...
  
  def cached_read_value(self): # -> defaultdict[Any, Any] | Any | list[Any] | object | None:
    ...
  
  def assign_sub(self, *args, **kwargs):
    ...
  
  def assign_add(self, *args, **kwargs):
    ...
  
  def assign(self, *args, **kwargs):
    ...
  
  @property
  def initializer(self):
    ...
  
  def initialized_value(self):
    ...
  
  @property
  def initial_value(self):
    ...
  
  @property
  def op(self) -> ops.Operation:
    ...
  
  def value(self): # -> defaultdict[Any, Any] | Any | list[Any] | object | None:
    ...
  
  def eval(self, session=...):
    ...
  
  @property
  def graph(self):
    ...
  
  @property
  def device(self):
    ...
  
  @property
  def shape(self):
    ...
  
  @property
  def synchronization(self):
    ...
  
  @property
  def name(self):
    ...
  
  @property
  def trainable(self):
    ...
  
  @property
  def dtype(self):
    ...
  
  @property
  def constraint(self):
    ...
  
  def __array__(self, dtype=...): # -> ndarray[_Shape, dtype[Any]]:
    ...
  
  def __complex__(self): # -> complex:
    ...
  
  def __int__(self) -> int:
    ...
  
  def __float__(self): # -> float:
    ...
  
  def numpy(self): # -> Any:
    ...
  
  def __str__(self) -> str:
    ...
  
  def __repr__(self): # -> str:
    ...
  


class PerWorkerVariable(resource_variable_ops.BaseResourceVariable):
  """A wrapper around unsynced variables created on workers.

  `PerWorkerVariable`s are variables that are stored on workers and not
  synchronized. A `PerWorkerVariable` is really a wrapper around multiple
  independent `Variable`s stored on independent worker machines. 
  `PerWorkerVariable` is currently only tested and supported when used with
  `ParameterServerStrategy`. A `PerWorkerVariable` can be created by creating a
  `Variable` within strategy scope and using the `per_worker_variable` flag,
  e.g.:

  ```
  with strategy.scope():
    var = tf.Variable(initial_value=0.0, per_worker_variable=True)
  ```

  The implementation modifies the graph to ensure that a worker's local version
  of the variable is used for computation at call time, while needing only one
  function trace and requiring no code changes beyond the `per_worker_variable`
  flag. `PerWorkerVariable`s can thus be treated like a standard `Variable`, but
  support is experimental and not all ops have been tested.

  All per-worker values can be retrieved and read into a list via
  `PerWorkerVariable.read_all()`.

  Caveats:
    - `PerWorkerVariable`s should not be used as direct inputs to a
      `tf.function`. That is, they should not appear in a tf.function header as
      an input argument. However they can still be read and manipulated in a
      `tf.function`.
    - The `shape` argument must be fully-defined (no `None` entries) or left
      empty. Partially-defined shapes are not yet supported.
    - Automatic control dependencies do not work with `PerWorkerVariable`s, so
      returning a `PerWorkerVariable` is not supported, and `read_all()` should 
      be used to retrieve values. (TODO: b/286052052)
    - `PerWorkerVariable`s should not be created within a `tf.function`.
  """
  def __init__(self, strategy, next_creator, **kwargs) -> None:
    ...
  
  @property
  def handle(self):
    ...
  
  def handle_call_time_value(self): # -> tuple[Callable[[], Any], PerWorkerVariableSpec]:
    """Returns a closure to run for a handle at call time and its spec.

    This function is called in self.handle to create a placeholder
    which returns a handle on some worker or on the coordinator.
    """
    ...
  
  def read_all(self): # -> list[Any]:
    """Synchronously read variables from all workers into a list of Tensors."""
    ...
  


class PerWorkerVariableSpec(tensor.TensorSpec):
  def __init__(self, value=..., name=...) -> None:
    ...
  
  def placeholder_value(self, placeholder_context):
    ...
  


class DistributedTable(lookup_ops.StaticHashTable):
  """A distributed StaticHashTable for ParameterServerStrategy.

  An instance of DistributedTable has copies of a StaticHashTable and its
  resource handle on the coordinator of each worker, created at the
  DistributedTable instance initialization time with initializers on each
  worker. Users can call methods on a DistributedTable as if it were a
  StaticHashTable, which leads to execution with the resource local to the
  consumer worker (or the coordinator, if calling from the coordinator). This
  implementation relies on the fact that the methods of StaticHashTable are
  queried with the resource handle (instead of the python object).

  Currently, at saving time, a DistributedTable is saved as a StaticHashTable on
  the coordinator, and restoring a DistributedTable from SavedModel is not
  supported.
  """
  def __init__(self, strategy, wrapped_creator) -> None:
    ...
  
  def __getattr__(self, attr): # -> Callable[..., object] | property | Any:
    ...
  
  def resource_handle_call_time_value(self): # -> tuple[Callable[[], Any], TypeSpec]:
    """Returns a closure to run for a resource handle at call time and its spec.

    This function is called in self.resource_handle to create a placeholder
    which returns a resource handle on some worker or on the coordinator.
    """
    ...
  
  @property
  def resource_handle(self):
    ...
  
  @property
  def is_distributed_table(self): # -> Literal[True]:
    ...
  
  def __tf_experimental_restore_capture__(self, concrete_function, internal_capture):
    ...
  


_local_resource_restore_context = ...
def get_current_local_resource_restore_context(): # -> Any | None:
  ...

@contextlib.contextmanager
def with_local_resource_restore_context(instance): # -> Generator[None, Any, None]:
  ...

class LocalResourceRestoreContext:
  """Class holding information of a distributed instance, e.g. StaticHashTable.

  Pairing use with context manager `with_local_resource_restore_context` allows
  operations under this context manager to conveniently gets information of a
  component of the `RestoredDistributedTable` (and other restored distributed
  `CapturableResource` if we're supporting their distribution in the future),
  instead of looking it up from the mapping of the worker-to-resource handle.
  This is especially useful when we know which instance the operations should
  execute with and the mapping is not available yet.
  """
  def __init__(self, instance) -> None:
    ...
  


class RestoredDistributedTable(DistributedTable):
  """A restored and distributed StaticHashTable for ParameterServerStrategy."""
  def __init__(self, strategy, wrapped_creator) -> None:
    ...
  
  def resource_handle_call_time_value(self): # -> tuple[Callable[[], Any], TypeSpec]:
    """Returns a closure to run for a resource handle at call time and its spec.

    This function is called in self.resource_handle to create a placeholder
    which returns a resource handle on some worker or on the coordinator.
    """
    ...
  
  def __setattr__(self, name, value): # -> None:
    ...
  


