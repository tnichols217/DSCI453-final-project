"""
This type stub file was generated by pyright.
"""

import contextlib
import threading
from tensorflow.python.util.tf_export import tf_export

"""Class implementing utilities used by tf.distribute.Strategy."""
@tf_export(v1=["distribute.get_loss_reduction"])
def get_loss_reduction(): # -> Literal[ReduceOp.SUM, ReduceOp.MEAN]:
  """`tf.distribute.ReduceOp` corresponding to the last loss reduction.

  Returns:
    `tf.distribute.ReduceOp` corresponding to the last loss reduction for
    estimator and v1 optimizer use case. `tf.distribute.ReduceOp.SUM` otherwise.
  """
  ...

def regroup(values, wrap_class=..., always_wrap=...): # -> list[list[Any] | Any | tuple[list[Any] | Any | Mapping[Any, Any] | DistributedVariable | CompositeTensor | PerReplica, ...] | Mapping[Any, Any] | DistributedVariable | CompositeTensor | PerReplica] | tuple[list[Any] | Any | Mapping[Any, Any] | DistributedVariable | CompositeTensor | PerReplica, ...] | Mapping[Any, Any] | DistributedVariable | CompositeTensor | PerReplica:
  """Makes a nest per-replica into a nest of PerReplica/Mirrored values.

  Args:
    values: Values to regroup
    wrap_class: Class that `values` be wrapped in.
    always_wrap: Always wrap the `values` in `wrap_class` even if the values
        are the same except for DistributeVariable.
  Returns:
    Wrapped `values`.
  """
  ...

def select_replica(replica_id, structured): # -> defaultdict[Any, Any] | Any | list[Any] | object | None:
  """Specialize a nest of regular & per-replica values for one replica."""
  ...

def select_replica_mirrored(replica_id, structured): # -> defaultdict[Any, Any] | Any | list[Any] | object | None:
  """Specialize a nest of regular & mirrored values for one replica."""
  ...

def assert_mirrored(structured): # -> None:
  """Raises if the structured is not composed of mirrored or regular values."""
  ...

def update_regroup(extended, updates, group): # -> defaultdict[Any, Any] | Any | list[Any] | object | list[list[Any] | Any | tuple[list[Any] | Any | Mapping[Any, Any] | DistributedVariable | CompositeTensor | PerReplica, ...] | Mapping[Any, Any] | DistributedVariable | CompositeTensor | PerReplica] | tuple[list[Any] | Any | Mapping[Any, Any] | DistributedVariable | CompositeTensor | PerReplica, ...] | Mapping[Any, Any] | DistributedVariable | CompositeTensor | PerReplica | None:
  """Regroup for an update, with dependencies to ensure all updates execute."""
  ...

def value_container(val): # -> CompositeTensor | DistributedVariable:
  """Returns the container that this per-replica `value` belongs to.

  Args:
    val: A value returned by `call_for_each_replica()` or a variable created in
      `scope()`.

  Returns:
    A container that `value` belongs to.
    If value does not belong to any container (including the case of
    container having been destroyed), returns the value itself.
  """
  ...

def is_distributed_variable(v): # -> Any | bool:
  """Determine if a variable is ds variable or TPU mirrored variable."""
  ...

def is_distributed_table(v): # -> Any | bool:
  """Determine if an object is a DistributedTable."""
  ...

def validate_colocate_distributed_variable(v, extended): # -> None:
  ...

def validate_colocate(v, extended): # -> None:
  ...

def create_mirrored_variable(strategy, real_mirrored_creator, class_mapping, policy_mapping, **kwargs):
  """Create distributed variables with given synchronization and aggregation."""
  ...

def is_mirrored(val): # -> Any | Literal[False]:
  ...

def is_sync_on_read(val): # -> bool:
  ...

class CachingScopeLocal(threading.local):
  """Class for maintaining thread local state for caching scope."""
  def __init__(self) -> None:
    ...
  
  def enter_scope(self): # -> None:
    ...
  
  def exit_scope(self): # -> None:
    ...
  
  def in_caching_scope(self): # -> bool:
    ...
  


caching_scope_local = ...
@contextlib.contextmanager
def cache_variable_reads(): # -> Generator[None, Any, None]:
  """Scope for caching variable reads for AggregatingVariable.

  The variable reads for AggregatingVariable inside this scope are cached. i.e.
  the first read of variable reads the value from possibly remote handle, but
  subsequent reads are returned using local cached value.

  For example:
  strategy = ParameterServerStrategy...
  with strategy.scope():
    # Variable v is of AggregatingVariable type with actual variable residing
    # on PS.
    v = tf.Variable(1.0)

  with distribute_utils.cache_variable_reads():
    v.read_value()  # Reads value 1.0
    v.assign(constant_op.constant(5.0))  # v changes to 5.0
    t1 = v.read_value()
    t2 = v.read_value()  # Both t1 & t2 return cached value 1.0 from local CPU.

  Notes about cache_variable_reads scope:
  1. Nesting of scope cache_variable_reads() is not supported
  2. And when caching scope is enabled, the thread enabling the cache and
    mirrored_run._MirroredReplicaThread threads spawned from it will have
    caching enabled.

  Yields:
    A context for caching variables.
  """
  ...

VARIABLE_POLICY_MAPPING = ...
VARIABLE_CLASS_MAPPING = ...
TPU_VARIABLE_POLICY_MAPPING = ...
TPU_VARIABLE_CLASS_MAPPING = ...
