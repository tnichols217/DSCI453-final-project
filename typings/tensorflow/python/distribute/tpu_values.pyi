"""
This type stub file was generated by pyright.
"""

from tensorflow.python.distribute import values

"""Various classes representing TPU distributed values.

Note that the tests are in values_test.py .

"""
_scatter_error_msg = ...
class TPUVariableMixin:
  """Mixin for TPU variables."""
  def __init__(self, *args, **kwargs) -> None:
    ...
  
  def __getattr__(self, name):
    ...
  
  def get(self):
    ...
  
  @property
  def handle(self): # -> Tensor:
    """The handle by which this variable can be accessed."""
    ...
  
  @property
  def device(self):
    ...
  
  def read_value(self): # -> Any:
    ...
  
  def value(self): # -> Any:
    ...
  
  @property
  def op(self): # -> DistributedVarOp:
    ...
  


class TPUDistributedVariable(TPUVariableMixin, values.DistributedVariable):
  """DistributedVariable subclass for TPUStrategy."""
  def assign_sub(self, value, use_locking=..., name=..., read_value=...):
    ...
  
  def assign_add(self, value, use_locking=..., name=..., read_value=...):
    ...
  
  def assign(self, value, use_locking=..., name=..., read_value=...):
    ...
  
  def scatter_sub(self, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_add(self, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_mul(self, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_div(self, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_min(self, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_max(self, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_update(self, sparse_delta, use_locking=..., name=...):
    ...
  


class TPUMirroredVariable(TPUVariableMixin, values.MirroredVariable):
  """Holds a map from replica to TPU variables whose values are kept in sync."""
  @property
  def device(self):
    ...
  
  def assign_sub(self, value, use_locking=..., name=..., read_value=...): # -> Any:
    ...
  
  def assign_add(self, value, use_locking=..., name=..., read_value=...): # -> Any:
    ...
  
  def assign(self, value, use_locking=..., name=..., read_value=...): # -> Any:
    ...
  
  def scatter_sub(self, *args, **kwargs):
    ...
  
  def scatter_add(self, *args, **kwargs):
    ...
  
  def scatter_max(self, *args, **kwargs):
    ...
  
  def scatter_min(self, *args, **kwargs):
    ...
  
  def scatter_mul(self, *args, **kwargs):
    ...
  
  def scatter_div(self, *args, **kwargs):
    ...
  
  def scatter_update(self, *args, **kwargs):
    ...
  


class TPULazyDistributedVariable(TPUDistributedVariable):
  """TPU Mirrored variable to be initialized lazily in a batch."""
  def assign_sub(self, value, use_locking=..., name=..., read_value=...):
    ...
  
  def assign_add(self, value, use_locking=..., name=..., read_value=...):
    ...
  
  def assign(self, value, use_locking=..., name=..., read_value=...):
    ...
  
  def read_value(self): # -> Any:
    ...
  


class TPUSyncOnReadVariable(TPUVariableMixin, values.SyncOnReadVariable):
  """Holds a map from replica to variables whose values are reduced on save."""
  def assign_sub(self, *args, **kwargs): # -> object | _dispatcher_for_no_op | Operation | None:
    ...
  
  def assign_add(self, *args, **kwargs): # -> object | _dispatcher_for_no_op | Operation | None:
    ...
  
  def assign(self, *args, **kwargs): # -> object | _dispatcher_for_no_op | Operation | None:
    ...
  


def assign_sub(var, value, use_locking=..., name=..., read_value=...):
  ...

def assign_add(var, value, use_locking=..., name=..., read_value=...):
  ...

def assign(var, value, use_locking=..., name=..., read_value=...):
  ...

class TPUOnWritePolicy(values.OnWritePolicy):
  """Policy defined for `tf.VariableSynchronization.ON_WRITE` synchronization.

  This policy is created when `synchronization` is set to
  `tf.VariableSynchronization.AUTO` or `tf.VariableSynchronization.ON_WRITE`.
  """
  def assign_sub(self, var, value, use_locking=..., name=..., read_value=...):
    ...
  
  def assign_add(self, var, value, use_locking=..., name=..., read_value=...):
    ...
  
  def assign(self, var, value, use_locking=..., name=..., read_value=...):
    ...
  
  def scatter_sub(self, var, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_add(self, var, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_max(self, var, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_min(self, var, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_mul(self, var, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_div(self, var, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_update(self, var, sparse_delta, use_locking=..., name=...):
    ...
  


class TPUOnReadPolicy(values.OnReadPolicy):
  """Policy defined for `tf.VariableSynchronization.ON_READ` synchronization.

  This policy is created when `synchronization` is set to
  `tf.VariableSynchronization.ON_READ` and `aggregation` is set to any of the
  values allowed by the `tf.VariableAggregation` enum such as `NONE`, `SUM`,
  `MEAN` or `ONLY_FIRST_REPLICA`when creating a `tf.Variable` in `tf.distribute`
  scope.
  """
  def assign_sub(self, var, *args, **kwargs): # -> object | _dispatcher_for_no_op | Operation | None:
    ...
  
  def assign_add(self, var, *args, **kwargs): # -> object | _dispatcher_for_no_op | Operation | None:
    ...
  
  def assign(self, var, *args, **kwargs): # -> object | _dispatcher_for_no_op | Operation | None:
    ...
  
  def scatter_sub(self, *args, **kwargs):
    ...
  
  def scatter_add(self, *args, **kwargs):
    ...
  
  def scatter_max(self, *args, **kwargs):
    ...
  
  def scatter_min(self, *args, **kwargs):
    ...
  
  def scatter_mul(self, *args, **kwargs):
    ...
  
  def scatter_div(self, *args, **kwargs):
    ...
  
  def scatter_update(self, *args, **kwargs):
    ...
  


