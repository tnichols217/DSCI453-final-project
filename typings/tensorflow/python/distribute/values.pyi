"""
This type stub file was generated by pyright.
"""

from typing import Optional
from tensorflow.python.framework import composite_tensor, dtypes, ops, tensor as tensor_lib, tensor_shape, type_spec
from tensorflow.python.ops import variables as variables_lib
from tensorflow.python.training.saving import saveable_object
from tensorflow.python.types import core, distribute as ds_types, trace

"""Various classes representing distributed values."""
def apply_aggregation_replica_context(value, aggregation, destinations): # -> Any:
  """Aggregate `value` to `destinations` as specified by `aggregation`."""
  ...

class DistributedValues(ds_types.DistributedValues):
  """Base class for representing distributed values."""
  def __init__(self, values) -> None:
    """Should only be called by subclass __init__."""
    ...
  
  def __str__(self) -> str:
    ...
  
  def __repr__(self): # -> str:
    ...
  


class DistributedDelegate(DistributedValues):
  """A map from device to values; acts as the same type as the values."""
  def __getattr__(self, name): # -> Any:
    ...
  
  @property
  def values(self): # -> tuple[Any, ...]:
    """Returns the per replica values."""
    ...
  
  def __add__(self, o):
    ...
  
  def __radd__(self, o):
    ...
  
  def __sub__(self, o):
    ...
  
  def __rsub__(self, o):
    ...
  
  def __mul__(self, o):
    ...
  
  def __rmul__(self, o):
    ...
  
  def __truediv__(self, o):
    ...
  
  def __rtruediv__(self, o):
    ...
  
  def __floordiv__(self, o):
    ...
  
  def __rfloordiv__(self, o):
    ...
  
  def __mod__(self, o):
    ...
  
  def __rmod__(self, o):
    ...
  
  def __lt__(self, o) -> bool:
    ...
  
  def __le__(self, o) -> bool:
    ...
  
  def __gt__(self, o) -> bool:
    ...
  
  def __ge__(self, o) -> bool:
    ...
  
  def __and__(self, o):
    ...
  
  def __rand__(self, o):
    ...
  
  def __or__(self, o):
    ...
  
  def __ror__(self, o):
    ...
  
  def __xor__(self, o):
    ...
  
  def __rxor__(self, o):
    ...
  
  def __getitem__(self, o):
    ...
  
  def __pow__(self, o, modulo=...):
    ...
  
  def __rpow__(self, o):
    ...
  
  def __invert__(self):
    ...
  
  def __neg__(self):
    ...
  
  def __abs__(self):
    ...
  
  def __div__(self, o): # -> _NotImplementedType:
    ...
  
  def __rdiv__(self, o): # -> _NotImplementedType:
    ...
  
  def __matmul__(self, o): # -> _NotImplementedType:
    ...
  
  def __rmatmul__(self, o): # -> _NotImplementedType:
    ...
  


class PerReplica(DistributedValues, composite_tensor.CompositeTensor, ds_types.PerReplica):
  """Holds a map from replica to unsynchronized values."""
  @property
  def values(self): # -> tuple[Any, ...]:
    """Returns the per replica values."""
    ...
  


class PerReplicaSpec(type_spec.TypeSpec):
  """Type specification for a `PerReplica`."""
  __slots__ = ...
  value_type = ...
  def __init__(self, *value_specs) -> None:
    ...
  


class Mirrored(DistributedDelegate, ds_types.Mirrored):
  """Holds a map from replica to values which are kept in sync."""
  ...


class DistributedVarOp:
  """A class that looks like `tf.Operation`."""
  def __init__(self, name, graph, traceback, typ) -> None:
    ...
  
  def __eq__(self, o) -> bool:
    ...
  
  def __hash__(self) -> int:
    ...
  


class DistributedVariableTraceType(trace.TraceType):
  """TraceType of DistributedVariable objects."""
  def __init__(self, distributed_variable) -> None:
    ...
  
  def is_subtype_of(self, other): # -> bool:
    ...
  
  def most_specific_common_supertype(self, others): # -> Self | None:
    ...
  
  def placeholder_value(self, placeholder_context=...): # -> Any:
    ...
  
  def to_tensors(self, value): # -> list[Any]:
    ...
  
  def cast(self, value, _):
    ...
  
  def __hash__(self) -> int:
    ...
  
  def __eq__(self, other) -> bool:
    ...
  


class DistributedVariable(DistributedDelegate, variables_lib.Variable, core.Tensor):
  """Holds a map from replica to variables."""
  def __init__(self, strategy, values, aggregation, var_policy=...) -> None:
    ...
  
  def __deepcopy__(self, memo): # -> Self:
    """Perform a deepcopy of the `DistributedVariable`.

    Unlike the deepcopy of a regular tf.Variable, this keeps the original
    strategy and devices of the `DistributedVariable`.  To avoid confusion
    with the behavior of deepcopy on a regular `Variable` (which does
    copy into new devices), we only allow a deepcopy of a `DistributedVariable`
    within its originating strategy scope.

    Args:
      memo: The memoization object for `deepcopy`.

    Returns:
      A deep copy of the current `DistributedVariable`.

    Raises:
      RuntimeError: If trying to deepcopy into a different strategy.
    """
    ...
  
  def is_initialized(self, name=...):
    """Identifies if all the component variables are initialized.

    Args:
      name: Name of the final `logical_and` op.

    Returns:
      The op that evaluates to True or False depending on if all the
      component variables are initialized.
    """
    ...
  
  @property
  def initializer(self): # -> object | _dispatcher_for_no_op | Operation | None:
    ...
  
  def initialized_value(self):
    ...
  
  @property
  def initial_value(self):
    ...
  
  @property
  def constraint(self):
    ...
  
  @property
  def graph(self):
    ...
  
  @property
  def name(self):
    ...
  
  @property
  def dtype(self):
    ...
  
  @property
  def shape(self):
    ...
  
  @property
  def synchronization(self):
    ...
  
  @property
  def aggregation(self): # -> VariableAggregation:
    ...
  
  @property
  def handle(self): # -> None:
    ...
  
  def eval(self, session=...): # -> Any:
    ...
  
  @property
  def device(self):
    ...
  
  @property
  def trainable(self):
    ...
  
  @property
  def distribute_strategy(self): # -> Any:
    ...
  
  def get_shape(self) -> tensor_shape.TensorShape:
    ...
  
  def to_proto(self, export_scope=...):
    ...
  
  @property
  def op(self) -> ops.Operation:
    ...
  
  def read_value(self): # -> defaultdict[Any, Any] | Any | list[Any] | object | None:
    ...
  
  def value(self):
    ...
  
  def numpy(self): # -> Any:
    ...
  
  def assign_sub(self, value, use_locking=..., name=..., read_value=...):
    ...
  
  def assign_add(self, value, use_locking=..., name=..., read_value=...):
    ...
  
  def assign(self, value, use_locking=..., name=..., read_value=...):
    ...
  
  def scatter_sub(self, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_add(self, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_mul(self, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_div(self, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_min(self, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_max(self, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_update(self, sparse_delta, use_locking=..., name=...):
    ...
  
  def __tf_tracing_type__(self, _): # -> DistributedVariableTraceType:
    ...
  
  def __tf_tensor__(self, dtype: Optional[dtypes.DType] = ..., name: Optional[str] = ...) -> tensor_lib.Tensor:
    ...
  
  @property
  def is_distributed_variable(self): # -> Literal[True]:
    ...
  
  def __tf_experimental_restore_capture__(self, concrete_function, internal_capture): # -> Self:
    ...
  


class _DistributedVariableSaveable(saveable_object.SaveableObject):
  """Class for defining how to restore a DistributedVariable."""
  def __init__(self, distributed_variable, primary_variable, name) -> None:
    ...
  
  def restore(self, restored_tensors, restored_shapes):
    """Restore the same value into all variables."""
    ...
  


class _MirroredSaveable(saveable_object.SaveableObject):
  """Class for defining how to restore a MirroredVariable."""
  def __init__(self, mirrored_variable, primary_variable, name) -> None:
    ...
  
  def restore(self, restored_tensors, restored_shapes): # -> object | _dispatcher_for_no_op | Operation | None:
    """Restore the same value into all variables."""
    ...
  


class MirroredVariable(DistributedVariable, Mirrored):
  """Holds a map from replica to variables whose values are kept in sync."""
  def scatter_min(self, *args, **kwargs):
    ...
  
  def scatter_max(self, *args, **kwargs):
    ...
  
  def scatter_update(self, *args, **kwargs):
    ...
  


class _SyncOnReadSaveable(saveable_object.SaveableObject):
  """Class for defining how to restore a SyncOnReadVariable."""
  def __init__(self, sync_on_read_variable, name) -> None:
    ...
  
  def restore(self, restored_tensors, restored_shapes): # -> object | _dispatcher_for_no_op | Operation | None:
    """Restore the same value into all variables."""
    ...
  


class SyncOnReadVariable(DistributedVariable):
  """Holds a map from replica to variables whose values are reduced on save."""
  def assign_sub(self, value, use_locking=..., name=..., read_value=...): # -> object | _dispatcher_for_no_op | Operation | None:
    ...
  
  def assign_add(self, value, use_locking=..., name=..., read_value=...): # -> object | _dispatcher_for_no_op | Operation | None:
    ...
  
  def assign(self, value, use_locking=..., name=..., read_value=...): # -> object | _dispatcher_for_no_op | Operation | None:
    ...
  
  def scatter_sub(self, *args, **kwargs): # -> None:
    ...
  
  def scatter_add(self, *args, **kwargs): # -> None:
    ...
  
  def scatter_mul(self, *args, **kwargs): # -> None:
    ...
  
  def scatter_div(self, *args, **kwargs): # -> None:
    ...
  
  def scatter_min(self, *args, **kwargs): # -> None:
    ...
  
  def scatter_max(self, *args, **kwargs): # -> None:
    ...
  
  def scatter_update(self, *args, **kwargs): # -> None:
    ...
  
  def value(self): # -> PackedVarAndDevice:
    ...
  
  def read_value(self): # -> defaultdict[Any, Any] | Any | list[Any] | object | None:
    ...
  


class VariablePolicy:
  """Policy defining synchronization and aggregation of a distributed variable.

  Given `synchronization` and `aggregation` parameters set on a `tf.Variable`
  during variable creation within `tf.distribute` scope, `tf.distribute` creates
  an appropriate policy object and assigns it to the distributed variable. All
  variable operations are delegated to the respective policy object.
  """
  def __init__(self, aggregation) -> None:
    ...
  
  def value(self):
    ...
  


class OnReadPolicy(VariablePolicy):
  """Policy defined for `tf.VariableSynchronization.ON_READ` synchronization.

  This policy is created when `synchronization` is set to
  `tf.VariableSynchronization.ON_READ` and `aggregation` is set to any of the
  values allowed by the `tf.VariableAggregation` enum such as `NONE`, `SUM`,
  `MEAN` or `ONLY_FIRST_REPLICA`when creating a `tf.Variable` in `tf.distribute`
  scope.
  """
  def value(self, var):
    ...
  
  def assign_sub(self, var, value, use_locking=..., name=..., read_value=...): # -> object | _dispatcher_for_no_op | Operation | None:
    """Subtracts a value from this variable."""
    ...
  
  def assign_add(self, var, value, use_locking=..., name=..., read_value=...): # -> object | _dispatcher_for_no_op | Operation | None:
    """Adds a value to this variable."""
    ...
  
  def assign(self, var, value, use_locking=..., name=..., read_value=...): # -> object | _dispatcher_for_no_op | Operation | None:
    ...
  
  def scatter_sub(self, *args, **kwargs): # -> None:
    ...
  
  def scatter_add(self, *args, **kwargs): # -> None:
    ...
  
  def scatter_mul(self, *args, **kwargs): # -> None:
    ...
  
  def scatter_div(self, *args, **kwargs): # -> None:
    ...
  
  def scatter_min(self, *args, **kwargs): # -> None:
    ...
  
  def scatter_max(self, *args, **kwargs): # -> None:
    ...
  
  def scatter_update(self, *args, **kwargs): # -> None:
    ...
  
  def get_saveable(self, var, primary_var, name): # -> tuple[Callable[[], Any], list[SaveSpec]]:
    """Create a saveable object for the given variable."""
    ...
  
  def get_restore_ops(self, var, tensor): # -> object | _dispatcher_for_no_op | Operation | None:
    """Restore the same value into all variables."""
    ...
  


class OnWritePolicy(VariablePolicy):
  """Policy defined for `tf.VariableSynchronization.ON_WRITE` synchronization.

  This policy is created when the following `synchronization` and `aggregation`
  parameters are specified when creating a `tf.Variable` in `tf.distribute`
  scope and `synchronization` is equal to `tf.VariableSynchronization.ON_WRITE`
  or `tf.VariableSynchronization.AUTO`.
  """
  def value(self, var):
    ...
  
  def assign(self, var, value, use_locking=..., name=..., read_value=...):
    ...
  
  def assign_add(self, var, value, use_locking=..., name=..., read_value=...):
    ...
  
  def assign_sub(self, var, value, use_locking=..., name=..., read_value=...):
    ...
  
  def scatter_sub(self, var, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_add(self, var, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_mul(self, var, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_div(self, var, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_min(self, var, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_max(self, var, sparse_delta, use_locking=..., name=...):
    ...
  
  def scatter_update(self, var, sparse_delta, use_locking=..., name=...):
    ...
  
  def get_saveable(self, var, primary_var, name): # -> tuple[Callable[[], Any | None], list[SaveSpec]]:
    """Saveable ops for AUTO variables."""
    ...
  
  def get_restore_ops(self, var, tensor): # -> object | _dispatcher_for_no_op | Operation | None:
    ...
  


class PerWorkerResource:
  """A per-worker CapturableResource class for non-ParameterServer strategy.

  Resources that populate `host_to_resources` should be instances of classes
  subclassing CapturableResource, although currently it's only used and tested
  for StaticHashTable with TPUStrategy.
  """
  def __init__(self, strategy, host_to_resources) -> None:
    ...
  
  def __getattribute__(self, name): # -> Any:
    ...
  
  def __setattr__(self, name, value): # -> None:
    ...
  
  def local_resource(self):
    """Returns the resource on the local worker."""
    ...
  


