"""
This type stub file was generated by pyright.
"""

"""Experimental library that exposes XLA operations directly in TensorFlow.

It is sometimes useful to be able to build HLO programs directly from
TensorFlow. This file provides Tensorflow operators that mirror the semantics of
HLO operators as closely as possible.

Note: Most of the operators defined in this module are used by the jax2tf
converter (see go/jax2tf for details) and are used in SavedModel produced
by jax2tf. Hence, we need to maintain backwards compatibility for these
operators. Please reach out to the JAX team if you want to make changes.
"""
_max = ...
_min = ...
_slice = slice
constant = ...
abs = ...
conj = ...
cos = ...
ceil = ...
digamma = ...
erf = ...
erfc = ...
erfinv = ...
ndtri = ...
exp = ...
expm1 = ...
floor = ...
imag = ...
is_finite = ...
lgamma = ...
log = ...
log1p = ...
logical_not = ...
neg = ...
real = ...
round = ...
sin = ...
sign = ...
tan = ...
tanh = ...
bessel_i0e = ...
bessel_i1e = ...
_SIGNED_TO_UNSIGNED_TABLE = ...
_UNSIGNED_TO_SIGNED_TABLE = ...
add = ...
sub = ...
mul = ...
div = ...
rem = ...
max = ...
min = ...
atan2 = ...
complex = ...
logical_and = ...
logical_or = ...
logical_xor = ...
eq = ...
ne = ...
ge = ...
gt = ...
le = ...
lt = ...
pow = ...
shift_left = ...
shift_right_logical = ...
shift_right_arithmetic = ...
igamma = ...
igamma_grad_a = ...
random_gamma_grad = ...
igammac = ...
polygamma = ...
zeta = ...
transpose = ...
rev = ...
bitcast_convert_type = ...
def broadcast(x, dims, name=...): # -> Any:
  ...

def clamp(a, x, b, name=...):
  ...

concatenate = ...
def conv(lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count=..., precision_config=..., preferred_element_type=..., name=..., use_v2=..., batch_group_count=...): # -> Any:
  """Wraps the XLA ConvGeneralDilated operator.

  ConvGeneralDilated is the most general form of XLA convolution and is
  documented at
  https://www.tensorflow.org/performance/xla/operation_semantics#conv_convolution

  Args:
    lhs: the input tensor
    rhs: the kernel tensor
    window_strides: the inter-window strides
    padding: the padding to apply at the start and end of each input dimensions
    lhs_dilation: dilation to apply between input elements
    rhs_dilation: dilation to apply between kernel elements
    dimension_numbers: a `ConvolutionDimensionNumbers` proto.
    feature_group_count: number of feature groups for grouped convolution.
    precision_config: a `xla.PrecisionConfig` proto.
    preferred_element_type: the result `dtype`.
    name: an optional name for the operator.
    use_v2: an optional request to use the XlaConvV2 op even if not necessary.
    batch_group_count: number of batch groups or grouped filters.

  Returns:
    A tensor representing the output of the convolution.
  """
  ...

convert_element_type = ...
def dot(lhs, rhs, name=...): # -> Any:
  ...

DotDimensionNumbers = ...
PrecisionConfig = ...
def dot_general(lhs, rhs, dimension_numbers, precision_config=..., preferred_element_type=..., name=..., use_v2=...): # -> Any:
  ...

def self_adjoint_eig(a, lower, max_iter, epsilon): # -> XlaSelfAdjointEig | _dispatcher_for_xla_self_adjoint_eig | object:
  ...

def svd(a, max_iter, epsilon, precision_config=...): # -> XlaSvd | _dispatcher_for_xla_svd | object:
  ...

dynamic_slice = ...
dynamic_update_slice = ...
einsum = ...
pad = ...
def random_normal(mu, sigma, dims, name=...):
  ...

def random_uniform(minval, maxval, dims, name=...): # -> Any:
  ...

def rng_bit_generator(algorithm, initial_state, shape, dtype): # -> XlaRngBitGenerator | _dispatcher_for_xla_rng_bit_generator | object:
  """Stateless PRNG bit generator.

  Wraps the XLA RngBitGenerator operator, documented at
    https://www.tensorflow.org/performance/xla/operation_semantics#rngbitgenerator.

  Args:
    algorithm: The PRNG algorithm to use, one of tf.random.Algorithm.{PHILOX,
      THREEFRY, AUTO_SELECT}.
    initial_state: Initial state for the PRNG algorithm. For THREEFRY, it should
      be a u64[2] and for PHILOX a u64[3].
    shape: The output shape of the generated data.
    dtype: The type of the tensor.

  Returns:
    a tuple with a new state and generated data of the given shape.
  """
  ...

recv = ...
reduce = ...
variadic_reduce = ...
def reduce_window(operand, init, reducer, window_dimensions, window_strides=..., base_dilations=..., window_dilations=..., padding=..., name=...): # -> Any:
  """Wraps the XLA ReduceWindow operator.

  ReduceWindow is documented at
  https://www.tensorflow.org/performance/xla/operation_semantics#reducewindow .

  Args:
    operand: the input tensor
    init: a scalar tensor representing the initial value for the reduction
    reducer: a reduction function that combines a pair of scalars.
    window_dimensions: shape of the window, as a list of integers
    window_strides: inter-window strides, as a list of integers. Optional; if
      omitted, defaults to strides of 1.
    padding: padding to apply to 'operand'. List of (low, high) pairs of
      integers that specify the padding to apply before and after each
      dimension. Optional; if omitted, defaults to no padding.
    name: the operator name, or None.

  Returns:
    A tensor that represents the output of the reduce_window operator.
  """
  ...

replica_id = ...
set_bound = ...
set_dynamic_dimension_size = ...
remove_dynamic_dimension_size = ...
def reshape(x, new_sizes, dimensions=..., name=...): # -> Any:
  ...

def select(condition, x, y, name=...): # -> Any:
  ...

select_and_scatter = ...
send = ...
def slice(x, start_dims, limit_dims, strides):
  ...

sharding = ...
spmd_full_to_shard_shape = ...
spmd_shard_to_full_shape = ...
sort = ...
key_value_sort = ...
variadic_sort = ...
while_loop = ...
dequantize = ...
custom_call = ...
def custom_call_v2(call_target_name, operands, result_specs, backend_config=..., has_side_effect=..., name=...): # -> object | _dispatcher_for_xla_custom_call_v2 | tuple[Any, ...] | list[Any]:
  """Emits an HLO `CustomCall` operation with multiple outputs.

  See `CustomCall` specification at
    https://tensorflow.org/xla/operation_semantics#customcall,
  and `mhlo.custom_call` specification at
    https://tensorflow.org/mlir/hlo_ops#mhlocustom_call_mlirmhlocustomcallop.

  Args:
    call_target_name: Name of the user function. The function signature must
      conform to version 3 of the API, see
      `API_VERSION_STATUS_RETURNING_UNIFIED`. All operands and results assumed
      to be in the default layout.
    operands: A sequence of tensors with possibly different types.
    result_specs: A sequence of tensor specs for all results.
    backend_config: A string that encodes a metadata for the backend. Empty
      string by default.
    has_side_effect: Indicates whether the custom call has side effects. `False`
      by default.
    name: Optional name of the operation.

  Returns:
    A tuple of output tensors.
  """
  ...

def call_module(args, *, version=..., module, Tout, Sout, platforms=..., function_list=..., has_token_input_output=..., disabled_checks=...): # -> tuple[()] | object | _dispatcher_for_xla_call_module | tuple[Any, ...] | list[Any]:
  """See documentation for the XlaCallModule op.

  https://github.com/search?q=repo%3Atensorflow%2Ftensorflow+path%3Axla_ops.cc+xlacallmodule&type=code
  """
  ...

def call_module_maximum_supported_version(): # -> Literal[9]:
  """Maximum version of XlaCallModule op supported.

  See versioning details documentation for the XlaCallModule op at:
  https://github.com/search?q=repo%3Atensorflow%2Ftensorflow+path%3Axla_call_module+%22int+kVersionMaximumSupported%22&type=code
  """
  ...

def call_module_disable_check_platform(): # -> Literal['platform']:
  ...

def gather(operand, start_indices, dimension_numbers, slice_sizes, indices_are_sorted=..., name=...): # -> Any:
  ...

def scatter(operand, scatter_indices, updates, update_computation, dimension_numbers, indices_are_sorted=..., name=...): # -> Any:
  ...

def optimization_barrier(*args): # -> object | _dispatcher_for_xla_optimization_barrier | tuple[Any, ...] | list[Any]:
  ...

def reduce_precision(operand, exponent_bits, mantissa_bits): # -> Any:
  ...

